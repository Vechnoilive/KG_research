# -*- coding: utf-8 -*-
"""SemanticGraph.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Iur_qyxhaO63xSjEWZcl4g50MjtkNSi_
"""

!pip install sentence-transformers spacy networkx tqdm pandas pyvis transformers
!python -m spacy download ru_core_news_sm

import os, re, json, pickle, math
from collections import defaultdict, Counter
from typing import List, Tuple
import numpy as np
import pandas as pd
import networkx as nx
from tqdm import tqdm
from sentence_transformers import SentenceTransformer
import spacy
from pyvis.network import Network
import random

DF_PATH = "df.tsv"
DFQ_PATH = "dfq.tsv"
CACHE_PATH = "graph_semantic_final_cache.pkl"
EVAL_OUT = "semantic_graph_eval_final.csv"
HTML_OUT = "graph_full_semantic_final.html"
MODEL_NAME = "deepvk/USER-base"
TOP_K_NEIGHBORS = 5

FRAG_SIM_THRESHOLD = 0.32
TRIPLET_SIM_THRESHOLD = 0.45

model = SentenceTransformer(MODEL_NAME)
VECTOR_DIM = model.get_sentence_embedding_dimension()
print(f"BERT loaded: {MODEL_NAME} dim={VECTOR_DIM}")

try:
    nlp = spacy.load("ru_core_news_sm")
    SPACY_OK = True
    print("spaCy Russian model loaded (ru_core_news_sm).")
except Exception:
    nlp = None
    SPACY_OK = False
    print("spaCy ru_core_news_sm not available ‚Äî using simple triplet fallback.")

# –ë–ª–æ–∫ 3

def encode_batch(texts: List[str]) -> np.ndarray:
    # –ü–æ–ª—É—á–∞–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤
    if not texts:
        return np.zeros((0, VECTOR_DIM), dtype=np.float32)
    embs = model.encode(texts, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=False)
    return embs.astype(np.float32)

def cos_sim(a: np.ndarray, b: np.ndarray) -> float:
    return float(np.dot(a, b))

def normalize_token(tok: str) -> str:
    return re.sub(r"[^–∞-—è—ëa-z0-9\-]", "", tok.lower())

def extract_keywords_simple(text: str, min_len: int = 4, top_n: int = 8) -> List[str]:
    toks = re.findall(r"\w+", text.lower(), flags=re.UNICODE)
    toks = [normalize_token(t) for t in toks if len(t) >= min_len]
    seen = set()
    out = []
    for t in toks:
        if not t or t in seen:
            continue
        seen.add(t)
        out.append(t)
        if len(out) >= top_n:
            break
    return out

def extract_triplets_spacy(text: str) -> List[Tuple[str,str,str]]:
    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç—Ä–∏–ø–ª–µ—Ç–æ–≤ (—Å—É–±—ä–µ–∫—Ç-–≥–ª–∞–≥–æ–ª-–æ–±—ä–µ–∫—Ç) —Å –ø–æ–º–æ—â—å—é spaCy
    if not SPACY_OK:
        return []
    doc = nlp(text)
    triplets = []
    for token in doc:
        if token.dep_.lower() in ("obj", "iobj", "dobj", "pobj") and token.head:
            subj = None
            for child in token.head.lefts:
                if child.dep_.lower().startswith("nsubj"):
                    subj = child.text
                    break
            if subj:
                triplets.append((normalize_token(subj), normalize_token(token.head.text), normalize_token(token.text)))
    seen = set(); out=[]
    for t in triplets:
        if t not in seen:
            seen.add(t); out.append(t)
    return out

def extract_triplets_fallback(text: str) -> List[Tuple[str,str,str]]:
    parts = re.split(r'[.!?]', text)
    out=[]
    for s in parts:
        words = re.findall(r'\w+', s)
        if len(words) >= 3:
            out.append((normalize_token(words[0]), normalize_token(words[1]), normalize_token(" ".join(words[2:5]))))
        if len(out) >= 3:
            break
    return out

def extract_triplets(text: str) -> List[Tuple[str,str,str]]:
    res = extract_triplets_spacy(text)
    if res:
        return res
    return extract_triplets_fallback(text)

# –ë–ª–æ–∫ 4

def build_graph(rebuild: bool = False):
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≥—Ä–∞—Ñ –∏–∑ –∫—ç—à–∞ –∏–ª–∏ —Å—Ç—Ä–æ–∏–º –∑–∞–Ω–æ–≤–æ
    if os.path.exists(CACHE_PATH) and not rebuild:
        try:
            with open(CACHE_PATH, "rb") as f:
                data = pickle.load(f)
            G, article_texts, VECTOR_DIM_CACHED = data
            if VECTOR_DIM_CACHED == VECTOR_DIM:
                print("Loaded graph from cache.")
                return G, article_texts
            else:
                print("Vector dim mismatch ‚Äî rebuilding graph.")
        except Exception as e:
            print("Cache load failed:", e)

    df = pd.read_csv(DF_PATH, sep="\t", dtype={"aid": str})
    assert {'aid', 'type', 'value'} <= set(df.columns), "df.tsv must contain columns 'aid','type','value'"

    print("Building SEMANTIC knowledge graph (from syntactic trees)...")
    G = nx.Graph()
    article_texts = {}

    all_entities = set()
    entity_vectors = {}

    # –°–æ–∑–¥–∞–Ω–∏–µ —É–∑–ª–æ–≤ —Å—Ç–∞—Ç–µ–π –∏ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
    for aid, group in tqdm(df.groupby("aid"), desc="Articles", total=len(df["aid"].unique())):
        art_node = f"art_{aid}"
        header_rows = group[group['type'] == 'header']
        title = header_rows['value'].iloc[0] if not header_rows.empty else f"–°—Ç–∞—Ç—å—è {aid}"
        art_vec = encode_batch([str(title)])[0]
        G.add_node(art_node, type='article', aid=aid, title=str(title), vector=art_vec)

        frag_texts = []
        frag_rows = group.reset_index(drop=True)
        frag_index = 0

        for _, row in frag_rows.iterrows():
            frag_text = str(row['value'])
            frag_type = row['type']
            frag_node = f"frag_{aid}_{frag_index}"
            frag_index += 1
            frag_vec = encode_batch([frag_text])[0]
            G.add_node(frag_node, type='fragment', text=frag_text, vector=frag_vec, frag_type=frag_type)
            G.add_edge(art_node, frag_node, relation='contains')
            frag_texts.append(frag_text)

            # --- –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ ---
            kws = extract_keywords_simple(frag_text, min_len=4, top_n=8)
            for kw in kws:
                kw_node = f"kw_{kw}"
                if not G.has_node(kw_node):
                    G.add_node(kw_node, type='keyword', name=kw)
                if not G.has_edge(frag_node, kw_node):
                    G.add_edge(frag_node, kw_node, relation='mentions')

            # --- –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Ç—Ä–∏–ø–ª–µ—Ç—ã ---
            trips = extract_triplets(frag_text)
            for t_i, (subj, rel, obj) in enumerate(trips):
                if not subj or not obj or not rel:
                    continue
                subj = subj.strip(); rel = rel.strip(); obj = obj.strip()
                if not subj or not obj or not rel:
                    continue

                # –î–æ–±–∞–≤–ª—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏
                all_entities.update([subj, obj])

                # –£–∑–µ–ª —Ç—Ä–∏–ø–ª–µ—Ç–∞
                tri_node = f"tri_{aid}_{frag_index}_{t_i}"
                tri_text = f"{subj} {rel} {obj}"
                tri_vec = encode_batch([tri_text])[0]
                G.add_node(tri_node, type='triple', text=tri_text, vector=tri_vec)
                G.add_edge(frag_node, tri_node, relation='has_triplet')

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–Ω—É—Ç—Ä–µ–Ω–Ω—é—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ç—Ä–∏–ø–ª–µ—Ç–∞
                G.add_node(subj, type='entity')
                G.add_node(obj, type='entity')
                G.add_edge(tri_node, subj, relation='subject_of', label=rel)
                G.add_edge(tri_node, obj, relation='object_of', label=rel)

        article_texts[aid] = " ".join(frag_texts)

    # --- –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å—É—â–Ω–æ—Å—Ç–µ–π ---
    print("Encoding entities for semantic merging...")
    entity_list = list(all_entities)
    if entity_list:
        entity_vecs = encode_batch(entity_list)
        for e, v in zip(entity_list, entity_vecs):
            entity_vectors[e] = v

    # --- –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å–∏–Ω–æ–Ω–∏–º–æ–≤ ---
    print("Merging semantically similar entities...")
    merged_map = {}
    used = set()
    for i, e1 in enumerate(entity_list):
        if e1 in used:
            continue
        cluster = [e1]
        v1 = entity_vectors.get(e1)
        if v1 is None or not np.any(v1):
            continue
        for j, e2 in enumerate(entity_list):
            if i == j or e2 in used:
                continue
            v2 = entity_vectors.get(e2)
            if v2 is None or not np.any(v2):
                continue
            sim = np.dot(v1, v2)
            if sim > 0.85:  # –ø–æ—Ä–æ–≥ –±–ª–∏–∑–æ—Å—Ç–∏
                cluster.append(e2)
                used.add(e2)
        main = cluster[0]
        for c in cluster:
            merged_map[c] = main


    # --- –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ ---
    print("Updating entity references after merging...")
    for e, canonical in merged_map.items():
        if e == canonical:
            continue
        if e in G.nodes and canonical in G.nodes:
            for n in list(G.neighbors(e)):
                data = G[e][n]
                if not G.has_edge(canonical, n):
                    G.add_edge(canonical, n, **data)
            G.remove_node(e)
        elif e in G.nodes and canonical not in G.nodes:
            G.nodes[e]['name'] = canonical

    # --- –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Ä—ë–±—Ä–∞ –º–µ–∂–¥—É –±–ª–∏–∑–∫–∏–º–∏ —Ç—Ä–∏–ø–ª–µ—Ç–∞–º–∏ ---
    print("Computing semantic neighbor edges (triplets)...")
    tri_nodes = [n for n, d in G.nodes(data=True) if d.get('type') == 'triple']
    tri_vecs = np.vstack([G.nodes[n]['vector'] for n in tri_nodes]) if tri_nodes else np.zeros((0, VECTOR_DIM))
    for i, n1 in enumerate(tri_nodes):
        v = tri_vecs[i]
        sims = tri_vecs @ v
        sims[i] = -1
        top_idx = np.argpartition(-sims, min(5, len(sims)-1))[:5]
        for j in top_idx:
            if sims[j] > TRIPLET_SIM_THRESHOLD:
                n2 = tri_nodes[j]
                if not G.has_edge(n1, n2):
                    sim = float(sims[j])
                    G.add_edge(n1, n2, relation='semantic', sim=sim, w=max(0.0001, 1.0 - sim))

    # --- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫—ç—à ---
    with open(CACHE_PATH, "wb") as f:
        pickle.dump((G, article_texts, VECTOR_DIM), f)

    print("Semantic knowledge graph built and cached.")
    return G, article_texts


G, article_texts = build_graph(rebuild=True)

def shortest_path_w_distance(G: nx.Graph, source: str, target: str, cutoff: int = 6) -> float:
    # –ö—Ä–∞—Ç—á–∞–π—à–∏–π –ø—É—Ç—å —Å —É—á–µ—Ç–æ–º –≤–µ—Å–∞ 'w'; –µ—Å–ª–∏ –ø—É—Ç–∏ –Ω–µ—Ç ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ–º inf
    try:
        return float(nx.shortest_path_length(G, source=source, target=target, weight='w'))
    except Exception:
        return float('inf')


def search_graph_combined(question: str, G: nx.Graph, article_texts: dict, top_k: int = 3):
    q_vec = encode_batch([question])[0]
    q_kws = set(extract_keywords_simple(question, min_len=3, top_n=12))

    # --- 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å—Ç–∞—Ç–µ–π –∏ –∏—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
    article_nodes = [n for n, d in G.nodes(data=True) if d.get("type") == "article"]
    article_kw_cache = {
        n: set(extract_keywords_simple(article_texts.get(G.nodes[n]["aid"], ""), min_len=3, top_n=20))
        for n in article_nodes
    }

    # --- 2. –ü—Ä—è–º–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ —Å—Ç–∞—Ç–µ–π —Å –≤–æ–ø—Ä–æ—Å–æ–º
    art_scores = {
        n: cos_sim(G.nodes[n].get("vector", np.zeros(VECTOR_DIM)), q_vec)
        for n in article_nodes
    }

    # --- 3. –ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–æ–ø–æ–≤—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤, —Ç—Ä–∏–ø–ª–µ—Ç–æ–≤ –∏ —Å—É—â–Ω–æ—Å—Ç–µ–π
    def get_top_nodes(node_type: str, limit: int = 50):
        nodes = [n for n, d in G.nodes(data=True) if d.get("type") == node_type and "vector" in d]
        if not nodes:
            return [], np.array([])
        vecs = np.vstack([G.nodes[n]["vector"] for n in nodes])
        sims = vecs @ q_vec
        top_idx = np.argpartition(-sims, min(limit, len(sims)-1))[:min(limit, len(sims))]
        return [nodes[i] for i in top_idx], sims

    top_frags, frag_sims = get_top_nodes("fragment")
    top_tris, tri_sims = get_top_nodes("triple")
    top_ents, ent_sims = get_top_nodes("entity")

    # --- 4. –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
    candidate_articles = set()
    for a, _ in sorted(art_scores.items(), key=lambda x: x[1], reverse=True)[:max(10, top_k * 5)]:
        candidate_articles.add(a)

    for nset in [top_frags[:30], top_tris[:30], top_ents[:30]]:
        for n in nset:
            for nbr in G.neighbors(n):
                if G.nodes[nbr].get("type") == "article":
                    candidate_articles.add(nbr)

    # --- 5. –û—Ü–µ–Ω–∫–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
    results = []
    SEMANTIC_RELS = {"subject_of", "object_of", "amod", "obl", "semantic_relation"}

    # –°—É—â–Ω–æ—Å—Ç–∏, —É–ø–æ–º—è–Ω—É—Ç—ã–µ –≤ –≤–æ–ø—Ä–æ—Å–µ
    entities_in_question = set(
        e for e, d in G.nodes(data=True)
        if d.get("type") == "entity" and e.lower() in question.lower()
    )

    for art in candidate_articles:
        direct = art_scores.get(art, 0.0)

        # --- –≤–∫–ª–∞–¥ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
        frag_sims_for_art = [
            cos_sim(G.nodes[f]["vector"], q_vec)
            for f in G.neighbors(art)
            if G.nodes[f].get("type") == "fragment"
        ]
        frag_contrib = np.mean(sorted(frag_sims_for_art, reverse=True)[:5]) if frag_sims_for_art else 0.0

        # --- –≤–∫–ª–∞–¥ –ø—É—Ç–∏ —á–µ—Ä–µ–∑ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Å–≤—è–∑–∏
        best_path_score = 0.0
        for src_set, sims, w in [(top_tris, tri_sims, 1.3), (top_frags, frag_sims, 1.0), (top_ents, ent_sims, 1.1)]:
            for src in src_set[:25]:
                # —Å–Ω–∞—á–∞–ª–∞ –ø—É—Ç—å –Ω–∞–ø—Ä—è–º—É—é
                path_len = shortest_path_w_distance(G, src, art, cutoff=6)
                if path_len != float("inf"):
                    path_score = 1.0 / (1.0 + path_len)
                    src_sim = float(np.dot(G.nodes[src]["vector"], q_vec))
                    score = path_score * max(1.0, src_sim * w)
                    if score > best_path_score:
                        best_path_score = score

                # –∞ —Ç–µ–ø–µ—Ä—å –ø—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Å–≤—è–∑–∏
                for nbr in G.neighbors(src):
                    rel = G[src][nbr].get("relation")
                    if rel in SEMANTIC_RELS:
                        path_len_sem = shortest_path_w_distance(G, nbr, art, cutoff=6)
                        if path_len_sem != float("inf"):
                            path_score = 1.0 / (1.0 + path_len_sem)
                            src_sim = float(np.dot(G.nodes[src]["vector"], q_vec))
                            score = 1.15 * path_score * max(1.0, src_sim * w)
                            if score > best_path_score:
                                best_path_score = score

        # --- –±–æ–Ω—É—Å –∑–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        kw_overlap = len(article_kw_cache[art] & q_kws)
        kw_bonus = min(1.0, 0.15 * kw_overlap)

        # --- –±–æ–Ω—É—Å –∑–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π –∏–∑ –≤–æ–ø—Ä–æ—Å–∞
        ent_bonus = 0.0
        for e in entities_in_question:
            path_len_ent = shortest_path_w_distance(G, e, art, cutoff=5)
            if path_len_ent != float("inf"):
                ent_bonus += 0.05 * (1.0 / (1.0 + path_len_ent))
        ent_bonus = min(0.25, ent_bonus)

        # --- —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–∫–æ—Ä
        final = (
            0.40 * direct +
            0.30 * frag_contrib +
            0.20 * best_path_score +
            0.05 * kw_bonus +
            0.05 * ent_bonus
        )

        results.append((art, final, {
            "direct": direct,
            "frag": frag_contrib,
            "path": best_path_score,
            "kw_bonus": kw_bonus,
            "ent_bonus": ent_bonus
        }))

    # --- —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –∏ –≤—ã–≤–æ–¥
    results.sort(key=lambda x: x[1], reverse=True)
    top_results = [{
        "aid": G.nodes[a]["aid"],
        "node": a,
        "title": G.nodes[a]["title"],
        "score": float(s),
        "meta": m
    } for a, s, m in results[:top_k]]

    print(f"\nQuery: {question!r}")
    if not top_results:
        print("   No results found.")
    else:
        for i, r in enumerate(top_results, 1):
            print(f"   {i}. AID={r['aid']} | {r['title'][:120]} | score={r['score']:.4f} | meta={r['meta']}")

    return top_results

# –ë–ª–æ–∫ 6 ‚Äî –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –≥—Ä–∞—Ñ–∞

HTML_OUT = "semantic_graph.html"

def visualize_graph(G, limit_nodes=300, save_path=HTML_OUT):
    print(f" –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≥—Ä–∞—Ñ–∞ (–¥–æ {limit_nodes} —É–∑–ª–æ–≤)...")
    net = Network(height="750px", width="100%", notebook=True, directed=True)

    # ‚úÖ –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ ‚Äî –≤ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–º JSON-—Ñ–æ—Ä–º–∞—Ç–µ
    net.set_options("""
    {
      "edges": {
        "smooth": false,
        "scaling": { "min": 0.1, "max": 0.1 },
        "color": { "color": "#888888", "highlight": "#555555", "hover": "#555555" },
        "width": 0.2
      },
      "nodes": {
        "scaling": { "min": 15, "max": 35 },
        "font": { "size": 20 }
      },
      "physics": {
        "enabled": true,
        "barnesHut": {
          "gravitationalConstant": -8000,
          "springLength": 250,
          "springConstant": 0.05,
          "avoidOverlap": 0.2
        }
      },
      "interaction": {
        "hover": true,
        "tooltipDelay": 200
      }
    }
    """)

    # –¶–≤–µ—Ç–∞ —É–∑–ª–æ–≤ –ø–æ —Ç–∏–ø–∞–º
    colors = {
        "article": "#ffcc00",   # –∂—ë–ª—Ç—ã–π
        "fragment": "#66ccff",  # –≥–æ–ª—É–±–æ–π
        "triple": "#ff9999",    # —Ä–æ–∑–æ–≤—ã–π
        "entity": "#b3ffb3",    # –∑–µ–ª—ë–Ω—ã–π
        "keyword": "#ffccff"    # —Å–∏—Ä–µ–Ω–µ–≤—ã–π
    }

    # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —É–∑–ª–æ–≤
    nodes = list(G.nodes)[:limit_nodes]
    subG = G.subgraph(nodes)

    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —É–∑–ª–æ–≤ ‚Äî –∫—Ä—É–ø–Ω—ã–µ, —Ü–≤–µ—Ç–Ω—ã–µ
    for n, d in subG.nodes(data=True):
        ntype = d.get("type", "unknown")
        label = d.get("title", d.get("text", d.get("name", n)))[:80]
        color = colors.get(ntype, "#cccccc")
        net.add_node(
            n,
            label=label,
            color=color,
            title=f"{ntype}: {label}"
        )

    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ä—ë–±–µ—Ä ‚Äî —Ç–æ–Ω–∫–∏–µ, —Å –ø–æ–¥–ø–∏—Å—è–º–∏ —Å–≤—è–∑–µ–π
    for u, v, data in subG.edges(data=True):
        if u in subG.nodes and v in subG.nodes:
            rel = data.get("relation", "—Å–≤—è–∑—å")
            sim = data.get("sim", 0.5)
            net.add_edge(
                u, v,
                label=rel,
                title=f"{rel} (sim={sim:.2f})"
            )

    net.show(save_path)
    print(f"‚úÖ –ì—Ä–∞—Ñ —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤: {save_path}")


# –ü—Ä–∏–º–µ—Ä –≤—ã–∑–æ–≤–∞
visualize_graph(G, limit_nodes=300)

# –ë–ª–æ–∫ 7
if __name__ == "__main__":

    os.makedirs(os.path.dirname(EVAL_OUT) or ".", exist_ok=True)

    if os.path.exists(DFQ_PATH):
        dfq = pd.read_csv(DFQ_PATH, sep="\t")
        rows = []
        correct_1 = 0
        correct_3 = 0

        # –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –≤—Å–µ–º –≤–æ–ø—Ä–æ—Å–∞–º –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞
        for _, row in tqdm(dfq.iterrows(), total=len(dfq), desc="Eval"):
            q = row['user_qu']
            true_aid = str(int(row['aid']))
            expected_list = (
                json.loads(row['aid_pred'])
                if isinstance(row['aid_pred'], str) and row['aid_pred'].startswith("[")
                else []
            )

            # –ü–æ–∏—Å–∫ –≤ –≥—Ä–∞—Ñ–µ
            res = search_graph_combined(q, G, article_texts, top_k=3)
            pred = res[0]['aid'] if res else None
            top3_aids = [r['aid'] for r in res] if res else []

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–ø–∞–¥–∞–Ω–∏–π
            ok1 = (pred == true_aid)
            ok3 = (true_aid in top3_aids)

            correct_1 += int(ok1)
            correct_3 += int(ok3)

            # –í—ã–≤–æ–¥ –æ–∂–∏–¥–∞–µ–º–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å—Ç–∞—Ç—å–∏
            exp_text = article_texts.get(true_aid, "")[:400].replace("\n", " ")
            print(f"\n EXPECTED TEXT ({true_aid}): {exp_text}\n")

            # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
            for i_r, r in enumerate(res, start=1):
                aid = r['aid']
                t = article_texts.get(aid, "")[:400].replace("\n", " ")
                print(f"üîπ RESULT {i_r}: AID={aid} TITLE={r['title'][:120]} ‚Üí {t}\n")

            rows.append({
                'question': q,
                'true': true_aid,
                'pred_top1': pred,
                'pred_top3': top3_aids,
                'ok@1': ok1,
                'ok@3': ok3,
                'expected_list': expected_list
            })

        # –ü–æ–¥—Å—á—ë—Ç Accuracy
        acc1 = (correct_1 / len(dfq) * 100) if len(dfq) > 0 else 0.0
        acc3 = (correct_3 / len(dfq) * 100) if len(dfq) > 0 else 0.0

        print(f"\n Accuracy@1 = {acc1:.2f}% ({correct_1}/{len(dfq)})")
        print(f" Accuracy@3 = {acc3:.2f}% ({correct_3}/{len(dfq)})")

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ–¥—Ä–æ–±–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        pd.DataFrame(rows).to_csv(EVAL_OUT, index=False)
        print("Detailed eval saved to", EVAL_OUT)
    else:
        print("dfq.tsv not found ‚Äî skipping evaluation.")

"""–ò–¥–µ—è: –≤ –∫–æ–Ω—Ü–µ –º–µ—Å—è—Ü–∞ –∏–ª–∏ –ø–æ–¥–æ–±–Ω–æ–≥–æ –≤–Ω–µ–¥—Ä—è—Ç—å —Å–æ–±—Ä–∞–Ω–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏ –≤–Ω–µ–¥—Ä—è—Ç—å –∏—Ö –≤ –≥—Ä–∞—Ñ –∑–Ω–∞–Ω–∏–π –¥–ª—è —É–≤–µ–ª–∏—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ –æ–±–ª–µ–≥—á–µ–Ω–∏—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
–¢–∞–∫–∂–µ –º–æ–∂–Ω–æ –ø–æ—Ä–æ–±–æ–≤–∞—Ç—å –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é: –Ω–æ –∏—Å–∫–∞—Ç—å –Ω–µ –≤ –æ–¥–Ω–æ–º –∫–ª–∞—Å—Ç–µ—Ä–µ, –Ω–æ –∏ –≤ —Å–æ—Å–µ–¥–Ω–∏—Ö —Ç–æ–∂–µ, —á—Ç–æ–±—ã —Ç–æ—á–Ω–æ –ø–æ–π–º–∞—Ç—å –∫—Ä–∞–π–Ω—é—é —Å—Ç–∞—Ç—å—é
"""

# –ø–æ–∫–∞–∑–∞—Ç—å –ø—Ä–∏–º–µ—Ä—ã —Ç—Ä–∏–ø–ª–µ—Ç–æ–≤
triples = [n for n, d in G.nodes(data=True) if d.get('type') == 'triple']
print("–ü—Ä–∏–º–µ—Ä —É–∑–ª–æ–≤-—Ç—Ä–∏–ø–ª–µ—Ç–æ–≤:", triples[:5])

# –ø–æ–∫–∞–∑–∞—Ç—å –∏—Ö —Å–æ—Å–µ–¥–µ–π (—Å—É–±—ä–µ–∫—Ç –∏ –æ–±—ä–µ–∫—Ç)
for t in triples[:3]:
    print(f"\n–¢—Ä–∏–ø–ª–µ—Ç: {G.nodes[t]['text']}")
    for nbr in G.neighbors(t):
        rel = G[t][nbr].get('relation')
        print(f"   ‚îú‚îÄ‚îÄ {rel} ‚Üí {nbr} ({G.nodes[nbr].get('type')})")