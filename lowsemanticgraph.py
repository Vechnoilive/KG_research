# -*- coding: utf-8 -*-
"""LowSemanticGraph.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rpQCJtAXd7kPDTju_iuwvU5mMWeTWbUt
"""

!pip install sentence-transformers spacy networkx tqdm pandas pyvis transformers
!python -m spacy download ru_core_news_sm

import os, re, json, pickle, math
from collections import defaultdict, Counter
from typing import List, Tuple
import numpy as np
import pandas as pd
import networkx as nx
from tqdm import tqdm
from sentence_transformers import SentenceTransformer
import spacy
from pyvis.network import Network
import random

DF_PATH = "df.tsv"
DFQ_PATH = "dfq.tsv"
CACHE_PATH = "graph_semantic_final_cache.pkl"
EVAL_OUT = "semantic_graph_eval_final.csv"
HTML_OUT = "graph_full_semantic_final.html"
MODEL_NAME = "deepvk/USER-base"
TOP_K_NEIGHBORS = 5

FRAG_SIM_THRESHOLD = 0.32
TRIPLET_SIM_THRESHOLD = 0.45

model = SentenceTransformer(MODEL_NAME)
VECTOR_DIM = model.get_sentence_embedding_dimension()
print(f"BERT loaded: {MODEL_NAME} dim={VECTOR_DIM}")

try:
    nlp = spacy.load("ru_core_news_sm")
    SPACY_OK = True
    print("spaCy Russian model loaded (ru_core_news_sm).")
except Exception:
    nlp = None
    SPACY_OK = False
    print("spaCy ru_core_news_sm not available ‚Äî using simple triplet fallback.")

# –ë–ª–æ–∫ 3

def encode_batch(texts: List[str]) -> np.ndarray:
    # –ü–æ–ª—É—á–∞–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤
    if not texts:
        return np.zeros((0, VECTOR_DIM), dtype=np.float32)
    embs = model.encode(texts, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=False)
    return embs.astype(np.float32)

def cos_sim(a: np.ndarray, b: np.ndarray) -> float:
    return float(np.dot(a, b))

def normalize_token(tok: str) -> str:
    return re.sub(r"[^–∞-—è—ëa-z0-9\-]", "", tok.lower())

def extract_keywords_simple(text: str, min_len: int = 4, top_n: int = 8) -> List[str]:
    toks = re.findall(r"\w+", text.lower(), flags=re.UNICODE)
    toks = [normalize_token(t) for t in toks if len(t) >= min_len]
    seen = set()
    out = []
    for t in toks:
        if not t or t in seen:
            continue
        seen.add(t)
        out.append(t)
        if len(out) >= top_n:
            break
    return out

def extract_triplets_spacy(text: str) -> List[Tuple[str,str,str]]:
    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç—Ä–∏–ø–ª–µ—Ç–æ–≤ (—Å—É–±—ä–µ–∫—Ç-–≥–ª–∞–≥–æ–ª-–æ–±—ä–µ–∫—Ç) —Å –ø–æ–º–æ—â—å—é spaCy
    if not SPACY_OK:
        return []
    doc = nlp(text)
    triplets = []
    for token in doc:
        if token.dep_.lower() in ("obj", "iobj", "dobj", "pobj") and token.head:
            subj = None
            for child in token.head.lefts:
                if child.dep_.lower().startswith("nsubj"):
                    subj = child.text
                    break
            if subj:
                triplets.append((normalize_token(subj), normalize_token(token.head.text), normalize_token(token.text)))
    seen = set(); out=[]
    for t in triplets:
        if t not in seen:
            seen.add(t); out.append(t)
    return out

def extract_triplets_fallback(text: str) -> List[Tuple[str,str,str]]:
    parts = re.split(r'[.!?]', text)
    out=[]
    for s in parts:
        words = re.findall(r'\w+', s)
        if len(words) >= 3:
            out.append((normalize_token(words[0]), normalize_token(words[1]), normalize_token(" ".join(words[2:5]))))
        if len(out) >= 3:
            break
    return out

def extract_triplets(text: str) -> List[Tuple[str,str,str]]:
    res = extract_triplets_spacy(text)
    if res:
        return res
    return extract_triplets_fallback(text)

# –ë–ª–æ–∫ 4

def build_graph(rebuild: bool = False):
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≥—Ä–∞—Ñ –∏–∑ –∫—ç—à–∞ –∏–ª–∏ —Å—Ç—Ä–æ–∏–º –∑–∞–Ω–æ–≤–æ
    if os.path.exists(CACHE_PATH) and not rebuild:
        try:
            with open(CACHE_PATH, "rb") as f:
                data = pickle.load(f)
            G, article_texts, VECTOR_DIM_CACHED = data
            if VECTOR_DIM_CACHED == VECTOR_DIM:
                print("Loaded graph from cache.")
                return G, article_texts
            else:
                print("Vector dim mismatch ‚Äî rebuilding graph.")
        except Exception as e:
            print("Cache load failed:", e)

    df = pd.read_csv(DF_PATH, sep="\t", dtype={"aid":str})
    assert {'aid','type','value'} <= set(df.columns), "df.tsv must contain columns 'aid','type','value'"

    print(" Building semantic graph...")
    G = nx.Graph()
    article_texts = {}

    # –°–æ–∑–¥–∞–Ω–∏–µ —É–∑–ª–æ–≤ —Å—Ç–∞—Ç–µ–π –∏ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
    for aid, group in tqdm(df.groupby("aid"), desc="Articles", total=len(df["aid"].unique())):
        art_node = f"art_{aid}"
        header_rows = group[group['type']=='header']
        title = header_rows['value'].iloc[0] if not header_rows.empty else f"–°—Ç–∞—Ç—å—è {aid}"
        art_vec = encode_batch([str(title)])[0]
        G.add_node(art_node, type='article', aid=aid, title=str(title), vector=art_vec)

        frag_texts = []
        frag_rows = group.reset_index(drop=True)
        frag_indices = 0
        for idx, row in frag_rows.iterrows():
            frag_text = str(row['value'])
            frag_type = row['type']
            frag_node = f"frag_{aid}_{frag_indices}"
            frag_indices += 1
            frag_vec = encode_batch([frag_text])[0]
            G.add_node(frag_node, type='fragment', text=frag_text, vector=frag_vec, frag_type=frag_type)
            G.add_edge(art_node, frag_node, relation='contains', sim=None, w=None)
            frag_texts.append(frag_text)

            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            kws = extract_keywords_simple(frag_text, min_len=4, top_n=8)
            for kw in kws:
                kw_node = f"kw_{kw}"
                if not G.has_node(kw_node):
                    G.add_node(kw_node, type='keyword', name=kw)
                if not G.has_edge(frag_node, kw_node):
                    G.add_edge(frag_node, kw_node, relation='mentions', sim=None, w=None)

            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç—Ä–∏–ø–ª–µ—Ç–æ–≤
            trips = extract_triplets(frag_text)
            for t_i, trip in enumerate(trips):
                subj, rel, obj = trip
                tri_node = f"tri_{aid}_{frag_indices}_{t_i}"
                tri_text = f"{subj} {rel} {obj}"
                tri_vec = encode_batch([tri_text])[0]
                G.add_node(tri_node, type='triple', text=tri_text, vector=tri_vec)
                G.add_edge(frag_node, tri_node, relation='has_triplet', sim=None, w=None)

        article_texts[aid] = " ".join(frag_texts)

    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –±–ª–∏–∑–æ—Å—Ç–∏ –º–µ–∂–¥—É —É–∑–ª–∞–º–∏ (—Ç–æ–ª—å–∫–æ —Ç–æ–ø-k —Å–æ—Å–µ–¥–µ–π)
    print("Computing semantic neighbor edges (top-k) to limit graph density...")
    nodes = list(G.nodes(data=True))
    node_list = [n for n,_ in nodes]
    vectors = [d.get('vector') for _,d in nodes]
    vecs = np.vstack([v if v is not None else np.zeros(VECTOR_DIM) for v in vectors]).astype(np.float32)
    N = len(node_list)

    for i in tqdm(range(N), desc="TopK neighbors"):
        v = vecs[i]
        if np.linalg.norm(v) == 0:
            continue
        sims = vecs @ v
        sims[i] = -1.0
        top_idx = np.argpartition(-sims, min(TOP_K_NEIGHBORS, len(sims)-1))[:TOP_K_NEIGHBORS]
        for j in top_idx:
            if sims[j] <= 0.25:
                continue
            n1 = node_list[i]; n2 = node_list[j]
            if G.has_edge(n1, n2):
                G[n1][n2]['sim'] = max(G[n1][n2].get('sim', 0.0) or 0.0, float(sims[j]))
            else:
                sim = float(sims[j])
                w = max(0.0001, 1.0 - sim)
                G.add_edge(n1, n2, relation='semantic', sim=sim, w=w)

    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∫–æ-–ø–æ—è–≤–ª–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
    print("Adding keyword cooccurrence edges...")
    kw_nodes = [n for n,d in G.nodes(data=True) if d.get('type')=='keyword']
    for i, a in enumerate(kw_nodes):
        frags_a = set(n for n in G.neighbors(a) if G.nodes[n]['type']=='fragment')
        for b in kw_nodes[i+1:]:
            frags_b = set(n for n in G.neighbors(b) if G.nodes[n]['type']=='fragment')
            inter = len(frags_a & frags_b)
            if inter > 0:
                sim = min(0.9, 0.2 + 0.2 * inter)
                w = max(0.0001, 1.0 - sim)
                if not G.has_edge(a,b):
                    G.add_edge(a,b, relation='cooccur', sim=sim, w=w)

    # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ sim –∏ w –¥–ª—è "contains" —Ä–µ–±–µ—Ä
    for u,v,data in list(G.edges(data=True)):
        if data.get('relation') == 'contains':
            vec_u = G.nodes[u].get('vector')
            vec_v = G.nodes[v].get('vector')
            if vec_u is not None and vec_v is not None and np.linalg.norm(vec_u) > 0 and np.linalg.norm(vec_v) > 0:
                sim = float(np.dot(vec_u, vec_v))
                G[u][v]['sim'] = sim
                G[u][v]['w'] = max(0.0001, 1.0 - sim)
            else:
                G[u][v]['sim'] = 0.0
                G[u][v]['w'] = 1.0

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ –≤ –∫—ç—à
    with open(CACHE_PATH, "wb") as f:
        pickle.dump((G, article_texts, VECTOR_DIM), f)
    print("Graph built and cached.")
    return G, article_texts

G, article_texts = build_graph(rebuild=True)

def shortest_path_w_distance(G: nx.Graph, source: str, target: str, cutoff: int = 6) -> float:
    # –ö—Ä–∞—Ç—á–∞–π—à–∏–π –ø—É—Ç—å —Å —É—á–µ—Ç–æ–º –≤–µ—Å–∞ 'w'; –µ—Å–ª–∏ –ø—É—Ç–∏ –Ω–µ—Ç ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ–º inf
    try:
        return float(nx.shortest_path_length(G, source=source, target=target, weight='w'))
    except Exception:
        return float('inf')


def search_graph_combined(question: str, G: nx.Graph, article_texts: dict, top_k: int = 3):
    q_vec = encode_batch([question])[0]
    q_kws = set(extract_keywords_simple(question, min_len=3, top_n=12))

    # 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å—Ç–∞—Ç–µ–π –∏ –∏—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
    article_nodes = [n for n, d in G.nodes(data=True) if d.get("type") == "article"]
    article_kw_cache = {
        n: set(extract_keywords_simple(article_texts.get(G.nodes[n]["aid"], ""), min_len=3, top_n=20))
        for n in article_nodes
    }

    # 2. –ü—Ä—è–º–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ —Å—Ç–∞—Ç–µ–π —Å –≤–æ–ø—Ä–æ—Å–æ–º
    art_scores = {
        n: cos_sim(G.nodes[n].get("vector", np.zeros(VECTOR_DIM)), q_vec)
        for n in article_nodes
    }

    # 3. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∏ —Ç—Ä–∏–ø–ª–µ—Ç–æ–≤
    def get_top_nodes(node_type: str, limit: int = 50):
        nodes = [n for n, d in G.nodes(data=True) if d.get("type") == node_type]
        vecs = np.vstack([G.nodes[n]["vector"] for n in nodes]) if nodes else np.zeros((0, VECTOR_DIM))
        sims = vecs @ q_vec if len(nodes) > 0 else np.zeros((0,))
        top_idx = np.argpartition(-sims, min(limit, len(sims)-1))[:min(limit, len(sims))] if len(sims) > 0 else []
        return [nodes[i] for i in top_idx], sims

    top_frags, frag_sims = get_top_nodes("fragment")
    top_tris, tri_sims = get_top_nodes("triple")

    #4. –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä—è–º–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞ –∏ —Å–æ—Å–µ–¥–µ–π
    candidate_articles = set()
    for a, _ in sorted(art_scores.items(), key=lambda x: x[1], reverse=True)[:max(10, top_k * 5)]:
        candidate_articles.add(a)
    for nset in [top_frags[:30], top_tris[:30]]:
        for n in nset:
            for nbr in G.neighbors(n):
                if G.nodes[nbr].get("type") == "article":
                    candidate_articles.add(nbr)

    # 5. –û—Ü–µ–Ω–∫–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
    results = []
    for art in candidate_articles:
        direct = art_scores.get(art, 0.0)

        # –í–∫–ª–∞–¥ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ —Å—Ç–∞—Ç—å–∏
        frag_sims_for_art = [
            cos_sim(G.nodes[f]["vector"], q_vec)
            for f in G.neighbors(art)
            if G.nodes[f].get("type") == "fragment"
        ]
        frag_contrib = np.mean(sorted(frag_sims_for_art, reverse=True)[:5]) if frag_sims_for_art else 0.0

        # –í–∫–ª–∞–¥ –ø—É—Ç–∏ –∏–∑ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∏ —Ç—Ä–∏–ø–ª–µ—Ç–æ–≤
        best_path_score = 0.0
        for src_set, sims, w in [(top_tris, tri_sims, 1.2), (top_frags, frag_sims, 1.0)]:
            for src in src_set[:20]:
                path_len = shortest_path_w_distance(G, src, art, cutoff=6)
                if path_len == float("inf"):
                    continue
                path_score = 1.0 / (1.0 + path_len)
                src_sim = float(np.dot(G.nodes[src]["vector"], q_vec))
                score = path_score * max(1.0, src_sim * w)
                if score > best_path_score:
                    best_path_score = score

        # –ë–æ–Ω—É—Å –∑–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        kw_overlap = len(article_kw_cache[art] & q_kws)
        kw_bonus = min(1.0, 0.15 * kw_overlap)

        # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
        final = (
            0.40 * direct +
            0.35 * frag_contrib +
            0.20 * best_path_score +
            0.05 * kw_bonus
        )

        results.append((art, final, {
            "direct": direct,
            "frag": frag_contrib,
            "path": best_path_score,
            "kw_bonus": kw_bonus
        }))

    results.sort(key=lambda x: x[1], reverse=True)
    top_results = [{
        "aid": G.nodes[a]["aid"],
        "node": a,
        "title": G.nodes[a]["title"],
        "score": float(s),
        "meta": m
    } for a, s, m in results[:top_k]]

    print(f"\ Query: {question!r}")
    if not top_results:
        print("    No results found.")
    else:
        for i, r in enumerate(top_results, 1):
            print(f"   {i}. AID={r['aid']} | {r['title'][:120]} | score={r['score']:.4f} | meta={r['meta']}")
    return top_results

# –ë–ª–æ–∫ 6 ‚Äî –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –≥—Ä–∞—Ñ–∞

HTML_OUT = "semantic_graph.html"

def visualize_graph(G, limit_nodes=300, save_path=HTML_OUT):
    print(f" –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≥—Ä–∞—Ñ–∞ (–¥–æ {limit_nodes} —É–∑–ª–æ–≤)...")
    net = Network(height="750px", width="100%", notebook=True, directed=False)
    net.toggle_physics(False)

    # –¶–≤–µ—Ç–∞ —É–∑–ª–æ–≤ –ø–æ —Ç–∏–ø—É
    colors = {
        "article": "#ffcc00",  # –∂–µ–ª—Ç—ã–π
        "fragment": "#66ccff", # –≥–æ–ª—É–±–æ–π
        "triple": "#ff9999"    # —Ä–æ–∑–æ–≤—ã–π
    }

    # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —á–∏—Å–ª–∞ —É–∑–ª–æ–≤ –¥–ª—è –Ω–∞–≥–ª—è–¥–Ω–æ—Å—Ç–∏
    nodes = list(G.nodes)[:limit_nodes]
    subG = G.subgraph(nodes)

    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —É–∑–ª–æ–≤
    for n, d in subG.nodes(data=True):
        ntype = d.get("type", "unknown")
        label = d.get("title", d.get("text", n))[:80]
        color = colors.get(ntype, "#cccccc")
        net.add_node(n, label=label, color=color)

    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ä—ë–±–µ—Ä
    for u, v, data in subG.edges(data=True):
        if u in subG.nodes and v in subG.nodes:
            net.add_edge(u, v, value=data.get("weight", 1.0))

    # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ñ–∏–∑–∏–∫–æ–π –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ HTML
    net.show_buttons(filter_=['physics'])
    net.show(save_path)
    print(f" –ì—Ä–∞—Ñ —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤: {save_path}")

visualize_graph(G, limit_nodes=400)

# –ë–ª–æ–∫ 7
if __name__ == "__main__":

    os.makedirs(os.path.dirname(EVAL_OUT) or ".", exist_ok=True)

    if os.path.exists(DFQ_PATH):
        dfq = pd.read_csv(DFQ_PATH, sep="\t")
        rows = []
        correct_1 = 0
        correct_3 = 0

        # –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –≤—Å–µ–º –≤–æ–ø—Ä–æ—Å–∞–º –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞
        for _, row in tqdm(dfq.iterrows(), total=len(dfq), desc="Eval"):
            q = row['user_qu']
            true_aid = str(int(row['aid']))
            expected_list = (
                json.loads(row['aid_pred'])
                if isinstance(row['aid_pred'], str) and row['aid_pred'].startswith("[")
                else []
            )

            # –ü–æ–∏—Å–∫ –≤ –≥—Ä–∞—Ñ–µ
            res = search_graph_combined(q, G, article_texts, top_k=3)
            pred = res[0]['aid'] if res else None
            top3_aids = [r['aid'] for r in res] if res else []

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–ø–∞–¥–∞–Ω–∏–π
            ok1 = (pred == true_aid)
            ok3 = (true_aid in top3_aids)

            correct_1 += int(ok1)
            correct_3 += int(ok3)

            # –í—ã–≤–æ–¥ –æ–∂–∏–¥–∞–µ–º–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å—Ç–∞—Ç—å–∏
            exp_text = article_texts.get(true_aid, "")[:400].replace("\n", " ")
            print(f"\n EXPECTED TEXT ({true_aid}): {exp_text}\n")

            # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
            for i_r, r in enumerate(res, start=1):
                aid = r['aid']
                t = article_texts.get(aid, "")[:400].replace("\n", " ")
                print(f"üîπ RESULT {i_r}: AID={aid} TITLE={r['title'][:120]} ‚Üí {t}\n")

            rows.append({
                'question': q,
                'true': true_aid,
                'pred_top1': pred,
                'pred_top3': top3_aids,
                'ok@1': ok1,
                'ok@3': ok3,
                'expected_list': expected_list
            })

        # –ü–æ–¥—Å—á—ë—Ç Accuracy
        acc1 = (correct_1 / len(dfq) * 100) if len(dfq) > 0 else 0.0
        acc3 = (correct_3 / len(dfq) * 100) if len(dfq) > 0 else 0.0

        print(f"\n Accuracy@1 = {acc1:.2f}% ({correct_1}/{len(dfq)})")
        print(f" Accuracy@3 = {acc3:.2f}% ({correct_3}/{len(dfq)})")

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ–¥—Ä–æ–±–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        pd.DataFrame(rows).to_csv(EVAL_OUT, index=False)
        print("Detailed eval saved to", EVAL_OUT)
    else:
        print("dfq.tsv not found ‚Äî skipping evaluation.")

"""–ò–¥–µ—è: –≤ –∫–æ–Ω—Ü–µ –º–µ—Å—è—Ü–∞ –∏–ª–∏ –ø–æ–¥–æ–±–Ω–æ–≥–æ –≤–Ω–µ–¥—Ä—è—Ç—å —Å–æ–±—Ä–∞–Ω–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏ –≤–Ω–µ–¥—Ä—è—Ç—å –∏—Ö –≤ –≥—Ä–∞—Ñ –∑–Ω–∞–Ω–∏–π –¥–ª—è —É–≤–µ–ª–∏—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ –æ–±–ª–µ–≥—á–µ–Ω–∏—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
–¢–∞–∫–∂–µ –º–æ–∂–Ω–æ –ø–æ—Ä–æ–±–æ–≤–∞—Ç—å –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é: –Ω–æ –∏—Å–∫–∞—Ç—å –Ω–µ –≤ –æ–¥–Ω–æ–º –∫–ª–∞—Å—Ç–µ—Ä–µ, –Ω–æ –∏ –≤ —Å–æ—Å–µ–¥–Ω–∏—Ö —Ç–æ–∂–µ, —á—Ç–æ–±—ã —Ç–æ—á–Ω–æ –ø–æ–π–º–∞—Ç—å –∫—Ä–∞–π–Ω—é—é —Å—Ç–∞—Ç—å—é
"""