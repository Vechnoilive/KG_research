# -*- coding: utf-8 -*-
"""ContextKeysGraph.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10PZsfkWl0rcg_QeSDyY0U0IlW6HLPkMV
"""

!pip install sentence-transformers spacy networkx tqdm pandas pyvis transformers
!python -m spacy download ru_core_news_sm

# -*- coding: utf-8 -*-
import os, re, json, math, random
from collections import defaultdict, Counter, deque
from typing import List, Tuple
import numpy as np
import pandas as pd
import networkx as nx
from tqdm import tqdm
from sentence_transformers import SentenceTransformer
import spacy
from pyvis.network import Network
import warnings
warnings.filterwarnings('ignore')

# === ÐŸÐ£Ð¢Ð˜ Ðš Ð¤ÐÐ™Ð›ÐÐœ ===
DF_PATH = "df.tsv"
DFQ_PATH = "dfq.tsv"
EVAL_OUT = "word_graph_eval.csv"
HTML_OUT = "word_graph.html"

# === ðŸ†• Ð£Ð›Ð£Ð§Ð¨Ð•ÐÐÐ«Ð• ÐŸÐÐ ÐÐœÐ•Ð¢Ð Ð« ===
MODEL_NAME = "deepvk/USER-base"
COOC_THRESHOLD = 2
SEMANTIC_SIM_THRESHOLD = 0.65
MAX_DEPTH = 3
MAX_NODES = 1200
TRIPLET_BOOST = 2.0
PROPERTY_BOOST = 1.5
COOC_BOOST = 1.2
SEMANTIC_DECAY = 0.85
DEGREE_PENALTY = 0.5
MERGE_THRESHOLD = 0.92
RERANK_TOP = 20
RERANK_WEIGHT = 0.8

# ÐœÐ¾Ð´ÐµÐ»Ð¸
model = SentenceTransformer(MODEL_NAME)
VECTOR_DIM = model.get_sentence_embedding_dimension()
print(f"âœ… USER model: {MODEL_NAME} (dim={VECTOR_DIM})")

try:
    nlp = spacy.load("ru_core_news_sm")
    SPACY_OK = True
    print("âœ… spaCy RU loaded!")
except:
    nlp = None
    SPACY_OK = False
    print("âš ï¸ spaCy fallback")

def encode_batch(texts: List[str]) -> np.ndarray:
    if not texts: return np.zeros((0, VECTOR_DIM), dtype=np.float32)
    return model.encode(texts, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=False)

def normalize_token(tok: str) -> str:
    """Normalize token: lowercase, remove punctuation, extra spaces."""
    if not tok:
        return ""
    tok = tok.lower().strip()
    tok = re.sub(r"[^Ð°-ÑÑ‘a-z0-9\- ]+", "", tok)
    return re.sub(r"\s+", " ", tok).strip()

def is_meaningful_word(w: str) -> bool:
    """Check if word is meaningful: length >=3, not numbers, not stopword."""
    if len(w) < 3 or re.fullmatch(r"[-0-9]+", w):
        return False
    stops = {
        "Ð¸", "Ð²", "Ð½Ð°", "Ð¿Ð¾", "Ñ", "Ðº", "Ð¾", "Ð¾Ð±", "Ð¾Ñ‚", "Ð´Ð¾", "Ð·Ð°", "Ñƒ",
        "Ð¶Ðµ", "Ð»Ð¸", "Ð´Ð°", "Ð½Ð¾", "Ð°", "ÐºÐ°Ðº", "Ñ‚Ð°Ðº", "Ñ‚Ð¾", "Ð¸Ð»Ð¸", "ÐµÑÐ»Ð¸",
        "Ñ‡Ñ‚Ð¾", "Ñ‡Ñ‚Ð¾Ð±Ñ‹", "ÑÑ‚Ð¾", "ÐµÐ³Ð¾", "ÐµÐµ", "Ð¸Ñ…", "Ð¼Ñ‹", "Ð²Ñ‹", "Ð¾Ð½", "Ð¾Ð½Ð°", "Ð¾Ð½Ð¸",
        "Ð¿Ñ€Ð¸", "Ð¸Ð·", "Ð±ÐµÐ·", "Ð¿Ñ€Ð¾", "Ð´Ð»Ñ", "Ð½Ð°Ð´", "Ð¿Ð¾Ð´", "Ð½Ðµ"
    }
    return w not in stops

def merge_nodes(G, a: str, b: str):
    """Merge node b into a: transfer aids, counts, edges."""
    if not (G.has_node(a) and G.has_node(b)) or a == b:
        return
    data_a, data_b = G.nodes[a], G.nodes[b]
    data_a["aids"] |= set(data_b.get("aids", set()))
    merged_counts = defaultdict(int, data_a.get("counts", {}))
    for aid, cnt in data_b.get("counts", {}).items():
        merged_counts[aid] += cnt
    data_a["counts"] = dict(merged_counts)

    for nbr in list(G.neighbors(b)):
        if nbr == a:
            continue
        edge_data = G.get_edge_data(b, nbr, default={}).copy()
        if not G.has_edge(a, nbr):
            G.add_edge(a, nbr, **edge_data)
        else:
            G[a][nbr]["weight"] = max(G[a][nbr].get("weight", 1.0), edge_data.get("weight", 1.0))
    G.remove_node(b)

def merge_similar_nodes(G, threshold: float = 0.88):  # Lowered to merge more forms
    """Merge semantically similar nodes (synonyms, word forms). Added logging for merges."""
    nodes = list(G.nodes)
    if len(nodes) < 2:
        return
    print(f"Merging similar nodes (threshold={threshold})...")
    embeddings = encode_batch(nodes)
    merged = set()
    merge_log = []  # Log merges
    for i in range(len(nodes)):
        if nodes[i] in merged:
            continue
        for j in range(i + 1, len(nodes)):
            if nodes[j] in merged:
                continue
            sim = float(np.dot(embeddings[i], embeddings[j]))
            if sim >= threshold:
                print(f"Merging '{nodes[j]}' into '{nodes[i]}' (sim={sim:.3f})")  # Log each merge
                merge_nodes(G, nodes[i], nodes[j])
                merged.add(nodes[j])
                merge_log.append((nodes[i], nodes[j], sim))
    print(f"Merged {len(merged)} nodes by similarity >= {threshold}. Log: {len(merge_log)} pairs.")
    # Optional: Save log to file for inspection
    if merge_log:
        pd.DataFrame(merge_log, columns=["target", "source", "sim"]).to_csv("merge_log.csv", index=False)
        print("Merge log saved to merge_log.csv")

def lemmatize_and_phrases(text: str, min_len: int = 3, top_n: int = 20) -> List[str]:
    """Extract lemmas from text (spaCy or fallback). Returns list of unique words."""
    text = str(text or "").strip()
    if not text:
        return []

    if SPACY_OK:
        doc = nlp(text)
        tokens = []
        for token in doc:
            if token.is_alpha and not token.is_stop:
                lemma = normalize_token(token.lemma_)
                if len(lemma) >= min_len and " " not in lemma:
                    tokens.append(lemma)
        freq = Counter(tokens)
        return [w for w, _ in freq.most_common(top_n)]

    # Fallback
    toks = re.findall(r"\w+", text.lower())
    toks = [normalize_token(t) for t in toks if len(t) >= min_len]
    freq = Counter(toks)
    return [w for w, _ in freq.most_common(top_n)]

def extract_triplets_spacy(text: str) -> List[Tuple[str, str, str]]:
    """Extract triplets (subj-verb-obj, noun-has_property-adj) using spaCy."""
    if not SPACY_OK:
        return []
    doc = nlp(text)
    triplets = []
    for token in doc:
        head = token.head
        # Subj-verb-obj
        if token.dep_ in ("obj", "dobj", "iobj", "pobj"):
            subj_cands = [c for c in head.children if c.dep_.startswith("nsubj")]
            if subj_cands:
                subj = normalize_token(subj_cands[0].lemma_)
                verb = normalize_token(head.lemma_)
                obj = normalize_token(token.lemma_)
                if all(len(s) > 2 for s in [subj, verb, obj]):
                    triplets.append((subj, verb, obj))
        # Noun-has_property-adj
        if token.pos_ == "ADJ" and head.pos_ in ("NOUN", "PROPN"):
            triplets.append((normalize_token(head.lemma_), "has_property", normalize_token(token.lemma_)))
        # Custom: "Ð¿Ð¾Ð´Ñ‚Ð²ÐµÑ€Ð´Ð¸Ñ‚ÑŒ" verb
        if head.lemma_ == "Ð¿Ð¾Ð´Ñ‚Ð²ÐµÑ€Ð´Ð¸Ñ‚ÑŒ" and token.dep_ in ("obj", "dobj"):
            subj_cands = [c for c in head.children if c.dep_.startswith("nsubj")]
            subj = normalize_token(subj_cands[0].lemma_) if subj_cands else ""
            triplets.append((subj or "user", "confirms", normalize_token(token.lemma_)))
    return list(set(triplets))

def extract_triplets_fallback(text: str) -> List[Tuple[str, str, str]]:
    """Fallback triplet extraction without spaCy."""
    sents = re.split(r"[.!?]", text)
    triplets = []
    for s in sents:
        words = re.findall(r"\w+", s.lower())
        if len(words) >= 3:
            subj = normalize_token(words[0])
            verb = normalize_token(words[1])
            obj = normalize_token(words[-1])
            if all(is_meaningful_word(w) for w in [subj, verb, obj]):
                triplets.append((subj, verb, obj))
    return triplets[:5]

def extract_triplets(text: str) -> List[Tuple[str, str, str]]:
    """Main triplet extraction: spaCy or fallback."""
    res = extract_triplets_spacy(text)
    if res:
        return res
    return extract_triplets_fallback(text)

def extract_cooc(text: str) -> List[Tuple[str, str]]:
    """Extract co-occurrence pairs of meaningful words in sentences."""
    sents = re.split(r"[.!?]+", text)
    pairs = []
    for sent in sents:
        words = lemmatize_and_phrases(sent, min_len=3)
        meaningful = [w for w in words if is_meaningful_word(w)]
        for i in range(len(meaningful)):
            for j in range(i + 1, len(meaningful)):
                pairs.append(tuple(sorted([meaningful[i], meaningful[j]])))
    return pairs

def build_word_graph(df_path: str = DF_PATH, sim_threshold: float = SEMANTIC_SIM_THRESHOLD):
    """Build enhanced word graph with triplets, cooc, semantic edges, IDF."""
    df = pd.read_csv(df_path, sep="\t", dtype={"aid": str})
    N_articles = len(df["aid"].unique())
    G = nx.Graph()
    print("Building enhanced graph: triplets + cooc + semantic...")

    word_aids = defaultdict(set)
    word_counts = defaultdict(lambda: defaultdict(int))
    triplet_edges = defaultdict(lambda: defaultdict(int))
    cooc_edges = defaultdict(int)

    for aid, group in tqdm(df.groupby("aid"), desc="Articles"):
        text = " ".join(group["value"].astype(str))
        tokens = lemmatize_and_phrases(text)

        # Words
        for w in tokens:
            if is_meaningful_word(w):
                word_aids[w].add(aid)
                word_counts[w][aid] += 1

        # Triplets
        for subj, rel, obj in extract_triplets(text):
            if all(is_meaningful_word(x) for x in [subj, rel, obj]):
                key = tuple(sorted([subj, obj]))
                triplet_edges[key][rel] += 1

        # Cooc
        for u, v in extract_cooc(text):
            cooc_edges[(u, v)] += 1

    # Nodes w/ IDF
    for w, aids in word_aids.items():
        if is_meaningful_word(w):
            df_count = len(aids)
            G.add_node(w, aids=set(aids), counts=dict(word_counts[w]),
                      df=df_count, idf=math.log(N_articles / max(1, df_count)))

    # Triplet edges
    for (u, v), rels in triplet_edges.items():
        if G.has_node(u) and G.has_node(v):
            G.add_edge(u, v, weight=sum(rels.values()), type="triplet", actions=dict(rels))

    # Cooc edges
    for (u, v), cnt in cooc_edges.items():
        if cnt >= COOC_THRESHOLD and G.has_node(u) and G.has_node(v):
            if G.has_edge(u, v):
                G[u][v]["weight"] += cnt * 0.5
                if "cooc" not in G[u][v].get("type_mix", []):
                    G[u][v].setdefault("type_mix", []).append("cooc")
            else:
                G.add_edge(u, v, weight=cnt * 0.3, type="cooc")

    # Semantic
    print("Adding semantic edges...")
    words = list(G.nodes)
    if words:
        embeddings = encode_batch(words)
        for i in range(len(words)):
            sims = np.dot(embeddings, embeddings[i])
            for j in range(i + 1, len(words)):
                s = sims[j]
                if s >= sim_threshold:
                    u, v = words[i], words[j]
                    if G.has_edge(u, v):
                        G[u][v]["weight"] = max(G[u][v]["weight"], s)
                    else:
                        G.add_edge(u, v, weight=s, type="semantic")

    # Merge
    merge_similar_nodes(G)

    # Recompute df/idf after merges
    for n in list(G.nodes):
        df_count = len(G.nodes[n]["aids"])
        G.nodes[n]["df"] = df_count
        G.nodes[n]["idf"] = math.log(N_articles / max(1, df_count))

    # Degrees
    for n in G.nodes:
        G.nodes[n]["degree"] = G.degree[n]

    print(f"Graph built: {len(G)} nodes | {len(G.edges)} edges")
    return G

def search_word_graph_multi_seed(G: nx.Graph, query: str, article_texts: dict):
    """Perform multi-seed search in graph with semantic seeds, BFS scoring, and text rerank."""
    print(f"\nContextual multi-seed search for query: '{query}'")

    # Semantic seeds: top-5 nodes by similarity to query
    words = list(G.nodes)
    if len(words) > 1000:
        words = random.sample(words, 1000)
    if not words:
        print("No nodes in graph.")
        return []
    emb_words = encode_batch(words)
    q_emb = model.encode([query])[0]
    sims = np.dot(emb_words, q_emb)
    top_idx = np.argsort(sims)[-5:][::-1]
    seed_words = [words[i] for i in top_idx]
    seed_sims = sims[top_idx]

    # Extract exact words from query and boost if present
    query_words = lemmatize_and_phrases(query, min_len=3)
    exact_seeds = [w for w in query_words if w in G.nodes and is_meaningful_word(w)]
    print(f"Exact query words: {', '.join(exact_seeds)}")

    for exact in exact_seeds:
        if exact in seed_words:
            idx = seed_words.index(exact)
            seed_sims[idx] *= 2.0  # Boost exact matches

    print(f"Seed words: {', '.join(seed_words)}")
    print(f"Visited seed words: {len(seed_words)}")

    if not seed_words:
        return []

    # Normalize seed weights
    word_weights = {w: float(s) for w, s in zip(seed_words, seed_sims)}
    total_w = sum(word_weights.values()) or 1.0
    word_weights = {w: ww / total_w for w, ww in word_weights.items()}

    # Multi-BFS with IDF and edge boosts
    total_scores = defaultdict(float)
    visited_global = set()

    for seed, weight in word_weights.items():
        queue = deque([(seed, 0, 1.0)])  # node, depth, path_weight
        visited = {seed}
        while queue:
            node, depth, pweight = queue.popleft()
            if len(visited_global) >= MAX_NODES or depth > MAX_DEPTH:
                continue

            node_data = G.nodes[node]
            idf = node_data.get("idf", 0)
            degree = max(1, node_data.get("degree", 1))
            decay = SEMANTIC_DECAY ** depth

            # Score articles from node
            for aid, cnt in node_data["counts"].items():
                art_len = max(1, len(article_texts.get(aid, "")))
                art_score = (cnt / art_len) * math.exp(idf) * (1 / degree ** DEGREE_PENALTY) * decay * pweight
                total_scores[aid] += art_score * weight

            # Process neighbors
            for nbr, edge_data in G[node].items():
                if nbr in visited:
                    continue
                eweight = edge_data.get("weight", 1.0)
                if eweight < 0.5:  # Skip weak edges
                    continue
                etype = edge_data.get("type", "semantic")
                boost = {
                    "triplet": TRIPLET_BOOST,
                    "cooc": COOC_BOOST,
                    "property": PROPERTY_BOOST
                }.get(etype, 1.0)
                if edge_data.get("actions", {}).get("has_property"):
                    boost *= PROPERTY_BOOST

                # Hub penalty for high-df nodes
                nbr_df = G.nodes[nbr].get("df", 1)
                hub_penalty = 0.5 if nbr_df > 10 else 1.0
                n_pweight = pweight * eweight * boost * decay * hub_penalty
                visited.add(nbr)
                visited_global.add(nbr)
                queue.append((nbr, depth + 1, n_pweight))

    if not total_scores:
        print("No articles scored.")
        return []

    # Normalize graph scores
    max_g = max(total_scores.values()) or 1.0
    graph_scores = {aid: sc / max_g for aid, sc in total_scores.items()}

    print("Top graph_scores (pre-rerank):")
    for aid, sc in sorted(graph_scores.items(), key=lambda x: x[1], reverse=True)[:3]:
        print(f"  {aid}: {sc:.3f}")

    # Rerank top candidates by text similarity
    candidates = sorted(graph_scores.items(), key=lambda x: x[1], reverse=True)[:RERANK_TOP]
    reranked = []
    for aid, gscore in candidates:
        text = article_texts[aid]  # Full text
        text_emb = model.encode([text])[0]
        tsim = float(np.dot(text_emb, q_emb))
        final_score = (1 - RERANK_WEIGHT) * gscore + RERANK_WEIGHT * tsim
        reranked.append((aid, final_score, article_texts[aid]))

    reranked.sort(key=lambda x: x[1], reverse=True)

    # Output top results
    print(f"Articles scored: {len(reranked)}")
    for i, (aid, sc, text) in enumerate(reranked[:5], 1):
        snippet = text[:250].replace("\n", " ")
        print(f"   -> AID={aid} | SCORE={sc:.3f} | {snippet}")

    return [{"aid": aid, "score": sc, "text": text} for aid, sc, text in reranked[:10]]

def visualize_graph(G, limit_nodes: int = 300, save_path: str = HTML_OUT):
    """Visualize graph with physics enabled (node repulsion, dynamic layout).
    Node sizes by df, edge colors by type.
    """
    print(f"Visualizing graph (up to {limit_nodes} nodes)...")

    import math
    from pyvis.network import Network

    # Subgraph selection
    if len(G) > limit_nodes:
        top_nodes = sorted(
            G.nodes,
            key=lambda n: (G.nodes[n].get("degree", 0), G.nodes[n].get("df", 0)),
            reverse=True
        )[:limit_nodes]
        subG = G.subgraph(top_nodes)
    else:
        subG = G

    net = Network(height="900px", width="100%", notebook=True, directed=False, cdn_resources='in_line')
    net.set_options("""
    {
      "edges": {
        "smooth": false,
        "color": { "color": "#888888", "highlight": "#555555", "hover": "#555555" },
        "width": 0.3
      },
      "nodes": {
        "borderWidth": 1,
        "font": { "size": 16, "face": "Arial", "align": "center" },
        "shape": "dot",
        "margin": 15
      },
      "physics": {
        "enabled": true,
        "stabilization": {
          "enabled": true,
          "iterations": 1000,
          "updateInterval": 50
        },
        "solver": "forceAtlas2Based",
        "forceAtlas2Based": {
          "gravitationalConstant": -30,
          "centralGravity": 0.005,
          "springLength": 150,
          "springConstant": 0.02,
          "avoidOverlap": 0.8
        }
      },
      "layout": {
        "randomSeed": 123,
        "improvedLayout": true
      },
      "interaction": {
        "hover": true,
        "tooltipDelay": 200,
        "dragNodes": true,
        "zoomView": true
      }
    }
    """)

    # Edge colors
    colors = {"triplet": "#ff9999", "semantic": "#66ccff", "cooc": "#99ff99", "mixed": "#b3ffb3"}

    # Add nodes
    for n, data in subG.nodes(data=True):
        df_val = data.get("df", 1)
        degree = data.get("degree", 1)
        aids = list(data.get("aids", []))[:3]
        title = (
            f"<b>{n}</b><br>"
            f"Articles: {len(data.get('aids', []))}<br>"
            f"Degree: {degree}<br>"
            f"AIDs: {', '.join(aids)}"
        )

        size = 15 + 10 * math.log1p(df_val)
        color = "#ffd966" if df_val <= 3 else "#ffb347" if df_val <= 6 else "#ff6666"
        net.add_node(n, label=n[:50], size=float(size), color=color, title=title)

    # Add edges
    for u, v, data in subG.edges(data=True):
        rel_type = data.get("type", "semantic")
        if "type_mix" in data:
            rel_type = "mixed"
        weight = float(data.get("weight", 0.5))
        color = colors.get(rel_type, "#cccccc")

        label = "triplet" if rel_type == "triplet" else ""
        title = f"{rel_type} (w={weight:.2f})"
        net.add_edge(u, v, label=label, color=color, title=title, width=0.1 + 0.05 * weight)

    # Save
    net.show(save_path)
    print(f"Graph saved to: {save_path}")

print("=== BUILDING GRAPH ===")
G = build_word_graph()

# Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° ÑÑ‚Ð°Ñ‚ÐµÐ¹
df = pd.read_csv(DF_PATH, sep="\t", dtype={"aid": str})
article_texts = df.groupby("aid")["value"].apply(lambda x: " ".join(x.astype(str))).to_dict()

# Ð’Ð¸Ð·ÑƒÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ
visualize_graph(G)

# ÐžÑ†ÐµÐ½ÐºÐ°
if not os.path.exists(DFQ_PATH):
    print("dfq.tsv not found. Skipping evaluation.")
else:
    dfq = pd.read_csv(DFQ_PATH, sep="\t")
    correct_1, correct_3 = 0, 0
    rows = []

    print("\n=== EVALUATION ===\n")

    for idx, row in tqdm(dfq.iterrows(), total=len(dfq), desc="Eval"):
        q = str(row["user_qu"])
        true_aid = str(row["aid"])

        # ÐŸÐ¾Ð¸ÑÐº Ñ‡ÐµÑ€ÐµÐ· Ð³Ñ€Ð°Ñ„
        res = search_word_graph_multi_seed(G, q, article_texts)
        if not res:
            print(f"âš ï¸ Ð’Ð¾Ð¿Ñ€Ð¾Ñ: {q[:60]}... â€” Ð½ÐµÑ‚ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð²\n")
            continue

        # Ð¡Ð¾Ñ€Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ Ð¿Ð¾ score
        res_sorted = sorted(res, key=lambda x: x["score"], reverse=True)
        top3 = res_sorted[:3]

        # ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ñ
        pred = top3[0]["aid"]
        top3_aids = [r["aid"] for r in top3]

        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° ÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ð¾ÑÑ‚Ð¸
        ok1 = (pred == true_aid)
        ok3 = (true_aid in top3_aids)
        correct_1 += int(ok1)
        correct_3 += int(ok3)

        # ------------------------------
        # ðŸ”¹ Ð’Ñ‹Ð²Ð¾Ð´ Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ° Ð¸ Ñ‚Ð¾Ð¿-3 ÑÑ‚Ð°Ñ‚ÐµÐ¹
        # ------------------------------
        print(f"ðŸ”¹ Ð’Ð¾Ð¿Ñ€Ð¾Ñ: {q}")
        print("-" * 90)


        # ÐžÐ¶Ð¸Ð´Ð°ÐµÐ¼Ð°Ñ ÑÑ‚Ð°Ñ‚ÑŒÑ
        if true_aid in article_texts:
            expected_text = article_texts[true_aid][:100].replace("\n", " ")
            print(f"\nâœ… ÐžÐ¶Ð¸Ð´Ð°ÐµÐ¼Ð°Ñ ÑÑ‚Ð°Ñ‚ÑŒÑ: AID={true_aid} | {expected_text}...\n")
        else:
            print(f"\nâš ï¸ ÐžÐ¶Ð¸Ð´Ð°ÐµÐ¼Ð°Ñ ÑÑ‚Ð°Ñ‚ÑŒÑ: AID={true_aid} Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð° Ð² df\n")

        # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ ÑÑ‚Ñ€Ð¾ÐºÑƒ Ð² Ð¾Ñ‚Ñ‡Ñ‘Ñ‚
        rows.append({
            "question": q,
            "true": true_aid,
            "pred": pred,
            "top3": ", ".join(top3_aids),
            "ok1": ok1,
            "ok3": ok3
        })

    # ------------------------------
    # ðŸ“ˆ Ð¤Ð¸Ð½Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸
    # ------------------------------
    acc1 = correct_1 / len(dfq) * 100
    acc3 = correct_3 / len(dfq) * 100

    print("=" * 90)
    print(f"ðŸŽ¯ Acc@1 = {acc1:.1f}% | Acc@3 = {acc3:.1f}%")
    print("=" * 90)

    pd.DataFrame(rows).to_csv(EVAL_OUT, index=False)
    print(f"âœ… Evaluation saved: {EVAL_OUT}")

print("Execution complete.")

def word_stats(G, word: str, article_texts: dict, top_n: int = 5):
    w = normalize_token(word)
    if w not in G:
        print(f"âŒ '{word}' not found")
        return
    node = G.nodes[w]
    counts = sorted(node["counts"].items(), key=lambda x: x[1], reverse=True)[:top_n]
    print(f"\nðŸ“Š '{word}': {node['df']} ÑÑ‚Ð°Ñ‚ÐµÐ¹ | IDF={node['idf']:.2f}")
    for i, (aid, cnt) in enumerate(counts, 1):
        print(f"{i}. {aid}: {cnt}x | {article_texts[aid][:150]}...")

word_stats(G, "ÑÐµÐ¼ÑŒÑ", article_texts)  # Test!
word_stats(G, "Ð±ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ð¾ÑÑ‚ÑŒ", article_texts)
