# -*- coding: utf-8 -*-
"""SyntaticGraph.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eIZufaDu__pouX9uBNWfWvcf0uTq4Qaj
"""

# Cell 1: –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π (–≤—ã–ø–æ–ª–Ω–∏—Ç—å –≤ Colab –æ–¥–∏–Ω —Ä–∞–∑)
!pip install -U pip setuptools wheel >/dev/null
!pip install spacy==3.6.0 networkx matplotlib pandas tqdm >/dev/null
!python -m spacy download ru_core_news_lg >/dev/null
!pip install pyvis >/dev/null
print("–£—Å—Ç–∞–Ω–æ–≤–∫–∏ –∑–∞–≤–µ—Ä—à–µ–Ω—ã.")

# Cell 2: –ò–º–ø–æ—Ä—Ç –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
import os
import json
from typing import List, Tuple, Dict, Any
import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt
from tqdm import tqdm
import spacy
from collections import defaultdict
import math
import random
from pyvis.network import Network  # –¥–ª—è –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–π HTML-–≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏

# –ü—É—Ç—å –∫ –≤—Ö–æ–¥–Ω–æ–º—É —Ñ–∞–π–ª—É (df.tsv –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤ /content/)
INPUT_PATH = "df.tsv"
OUT_DIR = "syntactic_graph_outputs"
os.makedirs(OUT_DIR, exist_ok=True)

# –ò—Å–∫–ª—é—á–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —á–∞—Å—Ç–∏ —Ä–µ—á–∏ –∏ –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –∏–∑ –≥—Ä–∞—Ñ–∞
EXCLUDED_POS = {"ADP", "CCONJ", "SCONJ", "PART", "PUNCT", "SPACE", "SYM", "INTJ"}

# –ó–∞–≥—Ä—É–∂–∞–µ–º –±–æ–ª—å—à—É—é —Ä—É—Å—Å–∫—É—é –º–æ–¥–µ–ª—å spaCy
nlp = spacy.load("ru_core_news_lg")

# –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ª–∏–º–∏—Ç –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–æ–ª—å—à–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
nlp.max_length = 2_000_000

print("spaCy model loaded:", nlp.pipe_names)

# Cell 3: –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞, –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ id –∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≥—Ä–∞—Ñ–∞
def parse_sentence_spacy(text: str):
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç tokens –∏ deps –¥–ª—è –æ–¥–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è:
    tokens: list of {idx (1-based), surface, lemma, pos}
    deps: list of {head, dep, label}
    –ò—Å–∫–ª—é—á–∞–µ—Ç —Å–ª—É–∂–µ–±–Ω—ã–µ —Å–ª–æ–≤–∞ (–ø—Ä–µ–¥–ª–æ–≥–∏, —Å–æ—é–∑—ã, —á–∞—Å—Ç–∏—Ü—ã, –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –∏ —Ç.–¥.).
    """
    doc = nlp(text)
    tokens = []
    deps = []

    # –°–Ω–∞—á–∞–ª–∞ —Å—Ç—Ä–æ–∏–º —Ç–∞–±–ª–∏—Ü—É –∏–Ω–¥–µ–∫—Å–æ–≤, –∏—Å–∫–ª—é—á–∞—è –Ω–µ–Ω—É–∂–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
    filtered = [tok for tok in doc if tok.pos_ not in EXCLUDED_POS]
    old_to_new = {tok.i: i + 1 for i, tok in enumerate(filtered)}  # spaCy idx -> 1-based –Ω–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å

    for i, tok in enumerate(filtered):
        tokens.append({
            "idx": i + 1,
            "surface": tok.text,
            "lemma": tok.lemma_,
            "pos": tok.pos_
        })

        # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å, –µ—Å–ª–∏ –≥–æ–ª–æ–≤–Ω–æ–µ —Å–ª–æ–≤–æ –Ω–µ –∏—Å–∫–ª—é—á–µ–Ω–æ
        if tok.head.i in old_to_new:
            head_idx = old_to_new[tok.head.i]
        else:
            head_idx = 0  # –µ—Å–ª–∏ –≥–æ–ª–æ–≤–∞ ‚Äî —Å–ª—É–∂–µ–±–Ω–æ–µ —Å–ª–æ–≤–æ –∏–ª–∏ root

        deps.append({
            "head": head_idx,
            "dep": i + 1,
            "label": tok.dep_
        })

    return tokens, deps

def clean_graph_remove_stopwords(G, nlp_model):
    """
    –£–¥–∞–ª—è–µ—Ç –∏–∑ –≥—Ä–∞—Ñ–∞ —É–∑–ª—ã-—Å–ª–æ–≤–∞, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞–º,
    —Å–ª—É–∂–µ–±–Ω—ã–º POS –∏ –Ω–µ–∞–ª—Ñ–∞–≤–∏—Ç–Ω—ã–º —Ç–æ–∫–µ–Ω–∞–º.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–æ–≤—ã–π –æ—á–∏—â–µ–Ω–Ω—ã–π –≥—Ä–∞—Ñ.
    """
    stop_pos = {"ADP", "CCONJ", "SCONJ", "PART", "PUNCT", "SYM"}
    stops = set(nlp_model.Defaults.stop_words)
    nodes_to_remove = []

    for n, data in G.nodes(data=True):
        if data.get("type") == "instance":
            lemma = str(data.get("lemma", "")).lower()
            pos = data.get("pos", "")
            surface = str(data.get("surface", "")).lower()
            if lemma in stops or pos in stop_pos or not surface.isalpha():
                nodes_to_remove.append(n)

    print(f"–£–¥–∞–ª—è–µ–º {len(nodes_to_remove)} —Å–ª—É–∂–µ–±–Ω—ã—Ö/—Å—Ç–æ–ø-—É–∑–ª–æ–≤ –∏–∑ {G.number_of_nodes()}")
    G_clean = G.copy()
    G_clean.remove_nodes_from(nodes_to_remove)
    return G_clean



def lemma_node_id(lemma: str, pos: str) -> str:
    """ID —É–∑–ª–∞ –ª–µ–º–º—ã (–æ–±—â–µ–≥–æ —É–∑–ª–∞ –ø–æ —Å–ª–æ–≤–æ—Ñ–æ—Ä–º–∞–º)"""
    return f"LEM::{lemma}::{pos}"


def instance_node_id(article_id: str, row_id: Any, sent_id: str, idx: int, surface: str) -> str:
    """ID —É–∑–ª–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –≤—Ö–æ–∂–¥–µ–Ω–∏—è —Å–ª–æ–≤–∞ –≤ —Ç–µ–∫—Å—Ç–µ"""
    safe = surface.replace(" ", "_")[:40]
    return f"INST::{article_id}::{row_id}::{sent_id}::{idx}::{safe}"


def sentence_node_id(article_id: str, sent_id: str) -> str:
    """ID —É–∑–ª–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è"""
    return f"SENT::{article_id}::{sent_id}"


def safe_str_for_graph(v):
    """–ü—Ä–∏–≤–æ–¥–∏—Ç —Å–ª–æ–∂–Ω—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∫ —Å–µ—Ä–∏–∞–ª–∏–∑—É–µ–º—ã–º —Å—Ç—Ä–æ–∫–∞–º (–¥–ª—è GraphML/GEXF)"""
    if isinstance(v, (list, dict, set)):
        return json.dumps(list(v) if isinstance(v, set) else v, ensure_ascii=False)
    return v

def compute_lemma_idf(G):
    lemma_to_articles = defaultdict(set)
    for n, d in G.nodes(data=True):
        if d.get("type") == "lemma":
            # —Å–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Å—Ç–∞—Ç—å–∏ –¥–ª—è –ª–µ–º–º—ã
            for u, v, data in G.in_edges(n, data=True):
                if data.get("type") == "instance_of":
                    aid = G.nodes[u].get("article_id")
                    if aid:
                        lemma_to_articles[d["lemma"]].add(aid)
    N = len({d.get("article_id") for n, d in G.nodes(data=True) if d.get("type") == "instance"})
    idf = {}
    for lemma, arts in lemma_to_articles.items():
        df = len(arts)
        idf[lemma] = math.log((N + 1) / (df + 1))
    return idf

def compute_lemma_pmi(G):
    # –ü–æ–¥—Å—á—ë—Ç —á–∞—Å—Ç–æ—Ç
    cooccur = defaultdict(int)
    lemma_freq = defaultdict(int)
    total_pairs = 0

    for u, v, data in G.edges(data=True):
        if data.get("type") == "lemma_dep":
            lu, lv = G.nodes[u].get("lemma"), G.nodes[v].get("lemma")
            if lu and lv:
                cooccur[(lu, lv)] += 1
                lemma_freq[lu] += 1
                lemma_freq[lv] += 1
                total_pairs += 1

    # –ï—Å–ª–∏ –Ω–µ—Ç –ø–∞—Ä ‚Äî –≤–µ—Ä–Ω—ë–º –ø—É—Å—Ç–æ–π —Å–ª–æ–≤–∞—Ä—å
    if total_pairs == 0:
        return {}

    # PMI –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤ –¥–∏–∞–ø–∞–∑–æ–Ω [0.5, 1.5]
    raw_pmi = {}
    min_pmi = float("inf")
    max_pmi = float("-inf")
    for (lu, lv), c in cooccur.items():
        p_uv = c / total_pairs
        p_u = max(1e-9, lemma_freq[lu] / total_pairs)
        p_v = max(1e-9, lemma_freq[lv] / total_pairs)
        val = math.log((p_uv / (p_u * p_v)) + 1e-9)
        raw_pmi[(lu, lv)] = val
        min_pmi = min(min_pmi, val)
        max_pmi = max(max_pmi, val)

    # –ï—Å–ª–∏ –≤—Å–µ PMI –æ–¥–∏–Ω–∞–∫–æ–≤—ã ‚Äî –≤–µ—Ä–Ω—É—Ç—å –±–∞–∑–æ–≤—ã–π –≤–µ—Å 1.0
    pmi_norm = {}
    if max_pmi - min_pmi < 1e-9:
        for k in raw_pmi:
            pmi_norm[k] = 1.0
        return pmi_norm

    # Min-max -> scale to [0.6, 1.4] (–≤–º–µ—Å—Ç–æ [0.5,1.5], –Ω–µ–º–Ω–æ–≥–æ –æ—Å—Ç–æ—Ä–æ–∂–Ω–µ–µ)
    low, high = 0.6, 1.4
    for k, v in raw_pmi.items():
        pmi_norm[k] = low + (v - min_pmi) * (high - low) / (max_pmi - min_pmi)
    return pmi_norm

# Cell 4: –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –º—É–ª—å—Ç–∏–≥—Ä–∞—Ñ–∞ –∏–∑ DataFrame
def build_syntactic_multigraph(df: pd.DataFrame,
                               type_col: str = "type",
                               value_col: str = "value",
                               article_col: str = "aid",
                               use_parsing_fn = parse_sentence_spacy):
    """
    –°—Ç—Ä–æ–∏—Ç networkx.MultiDiGraph –ø–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–º —Å—Ç—Ä–æ–∫–∞–º df.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (G, lemma_agg) –≥–¥–µ lemma_agg ‚Äî –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –º–µ–∂-–ª–µ–º–º–æ–≤—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π.

    –ò–∑–º–µ–Ω–µ–Ω–∏—è:
    - –¥–æ–±–∞–≤–ª–µ–Ω–∞ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Å–ª—É–∂–µ–±–Ω—ã—Ö —Å–ª–æ–≤ (—á–µ—Ä–µ–∑ parse_sentence_spacy)
    - –¥–æ–±–∞–≤–ª–µ–Ω –ø—Ä–æ–≥—Ä–µ—Å—Å –∏ –ø–æ–¥—Å—á—ë—Ç —Ç–æ–∫–µ–Ω–æ–≤
    - —É–ª—É—á—à–µ–Ω–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ –∞—Ç—Ä–∏–±—É—Ç–æ–≤ (safe_str_for_graph)
    """
    G = nx.MultiDiGraph()
    lemma_agg = {}  # key: (from_lem, to_lem, label) -> {count, doc_set, samples}

    # —Ñ–∏–ª—å—Ç—Ä —Å—Ç—Ä–æ–∫
    mask = df[type_col].str.lower().isin(["header", "sentence", "question"])
    df_sel = df[mask].copy()
    print("Rows selected for parsing:", len(df_sel))

    for row_idx, row in tqdm(df_sel.iterrows(), total=len(df_sel), desc="Parsing rows"):
        art_id = row.get(article_col, "A_unknown")
        text = row.get(value_col, "")
        if not str(text).strip():
            continue

        sent_unique_id = str(row_idx)

        # parse —á–µ—Ä–µ–∑ –Ω–∞—à—É —Ñ—É–Ω–∫—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è —É–∂–µ —Ñ–∏–ª—å—Ç—Ä—É–µ—Ç POS
        try:
            tokens, deps = use_parsing_fn(text)
        except Exception as e:
            print(f"‚ö†Ô∏è Parse error at row {row_idx}: {e}")
            continue

        # –µ—Å–ª–∏ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –Ω–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤ ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
        if not tokens:
            continue

        # —Å–æ–∑–¥–∞—ë–º —É–∑–µ–ª –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        # sent_nid = sentence_node_id(art_id, sent_unique_id)
        # if not G.has_node(sent_nid):
        #     G.add_node(
        #         sent_nid,
        #         type="sentence",
        #         article_id=art_id,
        #         sent_id=sent_unique_id,
        #         text=text,
        #     )

        # —Å–æ–∑–¥–∞—ë–º lemma —É–∑–ª—ã
        for tok in tokens:
            inst = instance_node_id(art_id, row_idx, sent_unique_id, tok["idx"], tok["surface"])
            lem = lemma_node_id(tok["lemma"], tok["pos"])

            if not G.has_node(inst):
                G.add_node(
                    inst,
                    type="instance",
                    surface=tok["surface"],
                    lemma=tok["lemma"],
                    pos=tok["pos"],
                    article_id=art_id,
                    sent_id=sent_unique_id,
                    tok_index=tok["idx"],
                )

            if not G.has_node(lem):
                G.add_node(
                    lem,
                    type="lemma",
                    lemma=tok["lemma"],
                    pos=tok["pos"],
                    doc_freq=0,
                    token_freq=0,
                )

            # —Å–≤—è–∑—å instance -> lemma
            G.add_edge(inst, lem, key="instance_of", type="instance_of")

            # –æ–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ª–µ–º–º—ã
            G.nodes[lem]["token_freq"] = G.nodes[lem].get("token_freq", 0) + 1
            G.nodes[lem].setdefault("doc_set", set()).add(art_id)

        # —Å–ª–æ–≤–∞—Ä—å —Ç–æ–∫–µ–Ω–æ–≤ –ø–æ –∏–Ω–¥–µ–∫—Å–∞–º
        inst_by_idx = {
            tok["idx"]: instance_node_id(art_id, row_idx, sent_unique_id, tok["idx"], tok["surface"])
            for tok in tokens
        }

        # —Å–≤—è–∑–∏ –ø–æ —Å–∏–Ω—Ç–∞–∫—Å–∏—Å—É
        for d in deps:
            head = d["head"]
            dep_idx = d["dep"]
            label = d.get("label", "dep")

            dep_inst = inst_by_idx.get(dep_idx)
            if head == 0:
                # —ç—Ç–æ –∫–æ—Ä–µ–Ω—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è ‚Äî –Ω–µ —Å–≤—è–∑—ã–≤–∞–µ–º —Å SENT —É–∑–ª–æ–º
                head_inst = None
            else:
                head_inst = inst_by_idx.get(head)

            if dep_inst is None or head_inst is None:
                continue


            occ = {
                "article_id": art_id,
                "row_idx": int(row_idx),
                "sent_id": sent_unique_id,
                "head_idx": head,
                "dep_idx": dep_idx,
            }

            G.add_edge(
                head_inst,
                dep_inst,
                key=f"dep::{label}",
                type="dep",
                label=label,
                occurrences=[safe_str_for_graph(occ)],
            )

            # –∞–≥—Ä–µ–≥–∏—Ä—É–µ–º –ª–µ–º–º–∞-–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
            def inst_to_lemma_id(node_id):
                if node_id.startswith("INST::"):
                    for _, v, _, ddata in G.out_edges(node_id, keys=True, data=True):
                        if ddata.get("type") == "instance_of":
                            return v
                return None

            from_lem = inst_to_lemma_id(head_inst)
            to_lem = inst_to_lemma_id(dep_inst)
            if from_lem and to_lem:
                key = (from_lem, to_lem, label)
                rec = lemma_agg.setdefault(key, {"count": 0, "doc_set": set(), "samples": []})
                rec["count"] += 1
                rec["doc_set"].add(art_id)
                if len(rec["samples"]) < 5:
                    rec["samples"].append(occ)

        # —Å–æ—Å–µ–¥–Ω–∏–µ —Å–≤—è–∑–∏ (–ø–æ –ø–æ–∑–∏—Ü–∏–∏)
        for i in range(len(tokens) - 1):
            id1 = inst_by_idx[tokens[i]["idx"]]
            id2 = inst_by_idx[tokens[i + 1]["idx"]]
            occ = {
                "article_id": art_id,
                "row_idx": int(row_idx),
                "sent_id": sent_unique_id,
                "i": tokens[i]["idx"],
            }
            G.add_edge(id1, id2, key="adj", type="adj", occurrences=[safe_str_for_graph(occ)])

    # —Ñ–∏–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º doc_freq
    for n in list(G.nodes):
        if G.nodes[n].get("type") == "lemma":
            doc_set = G.nodes[n].pop("doc_set", set())
            G.nodes[n]["doc_freq"] = len(doc_set)

    # –¥–æ–±–∞–≤–ª—è–µ–º lemma-level –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
    for (from_lem, to_lem, label), meta in lemma_agg.items():
        G.add_edge(
            from_lem,
            to_lem,
            key=f"lemma_dep::{label}",
            type="lemma_dep",
            label=label,
            count=meta["count"],
            doc_freq=len(meta["doc_set"]),
            samples=json.dumps(meta["samples"], ensure_ascii=False),
        )

    return G, lemma_agg

# Cell 5: –ó–∞–≥—Ä—É–∑–∫–∞ df.tsv –∏ –≤—ã–∑–æ–≤ —Å–±–æ—Ä–∫–∏ –≥—Ä–∞—Ñ–∞
# –ï—Å–ª–∏ —É —Ç–µ–±—è df.tsv –≤ –¥—Ä—É–≥–æ–º –º–µ—Å—Ç–µ, –ø–æ–ø—Ä–∞–≤—å INPUT_PATH –≤ –Ω–∞—á–∞–ª–µ
if not os.path.exists(INPUT_PATH):
    # –ø–æ–¥—Å–∫–∞–∑–∫–∞: –º–æ–∂–Ω–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∞–π–ª —Å –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –ü–ö
    from google.colab import files
    print("–ó–∞–≥—Ä—É–∑–∏—Ç–µ df.tsv (—Ñ–∞–π–ª –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∫–æ–ª–æ–Ω–∫–∏: aid, type, value).")
    uploaded = files.upload()
    # —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–µ—Ä–≤–æ–≥–æ —Ñ–∞–π–ª–∞ –∫–∞–∫ df.tsv
    for k in uploaded.keys():
        with open("/content/df.tsv","wb") as f:
            f.write(uploaded[k])
        break

df = pd.read_csv(INPUT_PATH, sep="\t", dtype=str).fillna("")
print("Loaded df shape:", df.shape)
print("Columns:", list(df.columns))

# –£–∫–∞–∂–∏ –∏–º–µ–Ω–∞ –∫–æ–ª–æ–Ω–æ–∫, –µ—Å–ª–∏ –æ–Ω–∏ –æ—Ç–ª–∏—á–∞—é—Ç—Å—è
type_col = "type"
value_col = "value"
article_col = "aid"  # –µ—Å–ª–∏ –≤ —Ç–≤–æ—ë–º —Ñ–∞–π–ª–µ –¥—Ä—É–≥–∞—è –∫–æ–ª–æ–Ω–∫–∞ ‚Äî –∏–∑–º–µ–Ω–∏ –∑–¥–µ—Å—å

G, lemma_agg = build_syntactic_multigraph(df, type_col=type_col, value_col=value_col, article_col=article_col)
LEMMA_IDF = compute_lemma_idf(G)
LEMMA_PMI = compute_lemma_pmi(G)

G_clean = clean_graph_remove_stopwords(G, nlp)


print("Graph built ‚Äî nodes:", G.number_of_nodes(), "edges:", G.number_of_edges())

def visualize_graph_html_fixed_draggable(G,
                                         output_html_path="syntactic_graph_fixed_draggable.html",
                                         max_nodes=20,
                                         seed=42):
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π HTML (pyvis) —Å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –Ω–∞—á–∞–ª—å–Ω—ã–º–∏ –ø–æ–∑–∏—Ü–∏—è–º–∏.
    ‚Ä¢ –û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç –¥–æ max_nodes —É–∑–ª–æ–≤ (–ø–æ —Å—Ç–µ–ø–µ–Ω–∏)
    ‚Ä¢ –£–∑–ª—ã –≤—Å–µ –æ–¥–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
    ‚Ä¢ –ë–µ–∑ —Ñ–∏–∑–∏–∫–∏ (–Ω–µ –¥–≤–∏–≥–∞—é—Ç—Å—è —Å–∞–º–∏)
    ‚Ä¢ –ú–æ–∂–Ω–æ –ø–µ—Ä–µ–º–µ—â–∞—Ç—å –º—ã—à–∫–æ–π (dragNodes=True)
    ‚Ä¢ –£–≤–µ–ª–∏—á–µ–Ω–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É –≤–µ—Ä—à–∏–Ω–∞–º–∏ √ó3
    """
    import math
    import networkx as nx
    from pyvis.network import Network

    if G.number_of_nodes() == 0:
        print("–ì—Ä–∞—Ñ –ø—É—Å—Ç–æ–π ‚Äî –Ω–µ—á–µ–≥–æ –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å.")
        return

    # 1Ô∏è‚É£ –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ø-N –ø–æ —Å—Ç–µ–ø–µ–Ω–∏
    top_nodes = sorted(G.degree(), key=lambda x: x[1], reverse=True)[:max_nodes]
    chosen_nodes = {n for n, _ in top_nodes}
    subG = G.subgraph(chosen_nodes).copy()
    print(f"–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–¥–≥—Ä–∞—Ñ–∞: {subG.number_of_nodes()} —É–∑–ª–æ–≤, {subG.number_of_edges()} —Ä—ë–±–µ—Ä")

    # 2Ô∏è‚É£ –°—Ç–∞—Ç–∏—á–µ—Å–∫–∞—è —Ä–∞—Å–∫–ª–∞–¥–∫–∞ (spring_layout)
    pos = nx.spring_layout(subG, seed=seed, iterations=300)
    PIXEL_SCALE = 1800.0  # ‚Üê –±—ã–ª–æ 600.0 (—É–≤–µ–ª–∏—á–µ–Ω–æ √ó3)
    MIN_DIST = 240.0      # ‚Üê –±—ã–ª–æ 80.0 (—É–≤–µ–ª–∏—á–µ–Ω–æ √ó3)
    MAX_ITERS = 200

    # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –≤ –ø–∏–∫—Å–µ–ª–∏
    xs = [p[0] for p in pos.values()]
    ys = [p[1] for p in pos.values()]
    minx, maxx = min(xs), max(xs)
    miny, maxy = min(ys), max(ys)
    width = max(1e-6, maxx - minx)
    height = max(1e-6, maxy - miny)
    pixel_pos = {}
    for n, (x, y) in pos.items():
        nxorm = (x - minx) / width
        nyorm = (y - miny) / height
        px = (nxorm - 0.5) * PIXEL_SCALE
        py = (nyorm - 0.5) * PIXEL_SCALE
        pixel_pos[n] = [px, py]

    # –ü—Ä–æ—Å—Ç–æ–µ —Ä–∞–∑–¥–≤–∏–≥–∞–Ω–∏–µ –±–ª–∏–∑–∫–∏—Ö –≤–µ—Ä—à–∏–Ω
    nodes_list = list(pixel_pos.keys())
    for it in range(MAX_ITERS):
        moved = False
        for i in range(len(nodes_list)):
            ni = nodes_list[i]
            xi, yi = pixel_pos[ni]
            for j in range(i + 1, len(nodes_list)):
                nj = nodes_list[j]
                xj, yj = pixel_pos[nj]
                dx = xi - xj
                dy = yi - yj
                dist = math.hypot(dx, dy)
                if dist < MIN_DIST and dist > 0:
                    overlap = (MIN_DIST - dist) / 2.0
                    ux = dx / dist
                    uy = dy / dist
                    pixel_pos[ni][0] += ux * overlap
                    pixel_pos[ni][1] += uy * overlap
                    pixel_pos[nj][0] -= ux * overlap
                    pixel_pos[nj][1] -= uy * overlap
                    moved = True
        if not moved:
            break

    # –¶–µ–Ω—Ç—Ä–∏—Ä—É–µ–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã
    all_x = [p[0] for p in pixel_pos.values()]
    all_y = [p[1] for p in pixel_pos.values()]
    mean_x, mean_y = sum(all_x) / len(all_x), sum(all_y) / len(all_y)
    for n in pixel_pos:
        pixel_pos[n][0] -= mean_x
        pixel_pos[n][1] -= mean_y

    # 3Ô∏è‚É£ –°–æ–∑–¥–∞—ë–º pyvis Network
    net = Network(
        directed=True,
        width="100%",
        height="1000px",  # —á—É—Ç—å –±–æ–ª—å—à–µ, —á—Ç–æ–±—ã –≤–º–µ—Å—Ç–∏—Ç—å
        bgcolor="#ffffff",
        font_color="#000000",
        notebook=False
    )

    # 4Ô∏è‚É£ –î–æ–±–∞–≤–ª—è–µ–º —É–∑–ª—ã ‚Äî –≤—Å–µ –æ–¥–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞, draggable=True
    node_size = 25
    for n, data in subG.nodes(data=True):
        label = data.get("lemma") or data.get("surface") or n.split("::")[-1]
        label = str(label)[:30]
        title = "<br>".join(f"<b>{k}</b>: {v}" for k, v in data.items() if k != "type")
        x, y = pixel_pos[n]
        net.add_node(
            n,
            label=label,
            title=title,
            x=int(x),
            y=int(y),
            size=node_size,
            fixed=False,   # –º–æ–∂–Ω–æ –¥–≤–∏–≥–∞—Ç—å
            physics=False, # –Ω–µ –¥–≤–∏–≥–∞—é—Ç—Å—è —Å–∞–º–∏
            font={"size": 16}
        )

    # 5Ô∏è‚É£ –î–æ–±–∞–≤–ª—è–µ–º —Ä—ë–±—Ä–∞
    for u, v, data in subG.edges(data=True):
        edge_label = data.get("label") or data.get("type") or ""
        net.add_edge(u, v, label=edge_label, arrows="to", smooth=False)

    # 6Ô∏è‚É£ –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
    net.set_options("""
    var options = {
      "nodes": {
        "font": {"size": 16, "face": "Arial"},
        "borderWidth": 1
      },
      "edges": {
        "color": {"inherit": true},
        "smooth": false
      },
      "interaction": {
        "hover": true,
        "dragNodes": true,
        "zoomView": true
      },
      "physics": {
        "enabled": false
      }
    }
    """)

    net.write_html(output_html_path)
    print(f"HTML-–≥—Ä–∞—Ñ —Å–æ—Ö—Ä–∞–Ω—ë–Ω (—Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ √ó3): {output_html_path}")

html_path = os.path.join(OUT_DIR, "syntactic_graph_fixed_draggable.html")
visualize_graph_html_fixed_draggable(G_clean, output_html_path=html_path, max_nodes=20)

# === Cell X: –ü–æ–ª–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ —Å–ª–æ–≤—É (–ª–µ–º–º–∞ –∏–ª–∏ –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–Ω–∞—è —Ñ–æ—Ä–º–∞) ===
def show_full_word_info(word: str, G: nx.MultiDiGraph, lemma_agg: dict):
    """
    –í—ã–≤–æ–¥–∏—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ —Å–ª–æ–≤—É:
      ‚Ä¢ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —É–∑–ª—ã (instance / lemma)
      ‚Ä¢ –∏—Ö –∞—Ç—Ä–∏–±—É—Ç—ã –∏ —Å–≤—è–∑–∏
      ‚Ä¢ —Å—Ç–∞—Ç—å–∏, –≥–¥–µ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –ª–µ–º–º–∞
      ‚Ä¢ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∏–∑ lemma_agg
    """
    word_lower = word.lower()
    matches = [
        n for n, d in G.nodes(data=True)
        if str(d.get("lemma", "")).lower() == word_lower or
           str(d.get("surface", "")).lower() == word_lower
    ]

    if not matches:
        print(f"–°–ª–æ–≤–æ '{word}' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ –≥—Ä–∞—Ñ–µ.")
        return

    print(f"–ù–∞–π–¥–µ–Ω–æ {len(matches)} —É–∑–ª–æ–≤ –¥–ª—è '{word}'.\n")

    lemma_prefix = f"LEM::{word_lower}::"
    total_deps = 0
    docs = set()
    dep_by_label = {}

    # –°–±–æ—Ä —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏–∑ lemma_agg
    for (fr, to, lbl), meta in lemma_agg.items():
        if lemma_prefix in fr or lemma_prefix in to:
            total_deps += meta.get("count", 0)
            docs.update(meta.get("doc_set", set()))
            other = to if lemma_prefix in fr else fr
            lemma_other = other.split("::")[1]
            dep_by_label.setdefault(lbl, []).append(lemma_other)

    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ –æ—Ç–¥–µ–ª—å–Ω—ã–º —É–∑–ª–∞–º (–ø–µ—Ä–≤—ã–µ 5)
    for i, node in enumerate(matches[:5], 1):
        data = G.nodes[node]
        print(f"=== –£–∑–µ–ª {i}/{min(len(matches), 5)}: {node} ===")
        for k, v in data.items():
            if isinstance(v, (set, list, dict)):
                v = json.dumps(v, ensure_ascii=False)[:200]
            print(f"  {k:12s}: {v}")

        in_edges = list(G.in_edges(node, keys=True, data=True))
        out_edges = list(G.out_edges(node, keys=True, data=True))
        print(f"\n–í—Ö–æ–¥—è—â–∏—Ö —Ä—ë–±–µ—Ä: {len(in_edges)}, –∏—Å—Ö–æ–¥—è—â–∏—Ö: {len(out_edges)}")

        if out_edges:
            print("\n–ò—Å—Ö–æ–¥—è—â–∏–µ —Å–≤—è–∑–∏ (–¥–æ 10):")
            for _, v, k, d in out_edges[:10]:
                lbl = d.get("label") or d.get("type", "")
                target = G.nodes[v].get("lemma") or G.nodes[v].get("surface") or v.split("::")[-1]
                print(f"  ‚Üí {target[:30]} [{lbl}]")

        if in_edges:
            print("\n–í—Ö–æ–¥—è—â–∏–µ —Å–≤—è–∑–∏ (–¥–æ 10):")
            for u, _, k, d in in_edges[:10]:
                lbl = d.get("label") or d.get("type", "")
                src = G.nodes[u].get("lemma") or G.nodes[u].get("surface") or u.split("::")[-1]
                print(f"  ‚Üê {src[:30]} [{lbl}]")

        print("\n" + "‚Äî" * 70 + "\n")

    # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ª–µ–º–º–µ
    print(f"–õ–µ–º–º–∞: {word}")
    if total_deps == 0:
        print("–ù–µ—Ç –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π (—Å–ª–æ–≤–æ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω–æ).")
        return

    print(f"–í—Å–µ–≥–æ —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π: {total_deps}")
    print(f"–í—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤ —Å—Ç–∞—Ç—å—è—Ö: {len(docs)}")
    if docs:
        sample = ", ".join(sorted(docs)[:10])
        extra = " ..." if len(docs) > 10 else ""
        print(f"  –ü—Ä–∏–º–µ—Ä—ã: {sample}{extra}")

    print("\n–¢–∏–ø—ã –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π:")
    for lbl, targets in dep_by_label.items():
        uniq = sorted(set(targets))
        sample = ", ".join(uniq[:5])
        extra = " ..." if len(uniq) > 5 else ""
        print(f"  {lbl:12s} ‚Üí {len(uniq)} –ª–µ–º–º: {sample}{extra}")


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
show_full_word_info("—Å–µ–º—å—è", G, lemma_agg)

# === –§–∏–Ω–∞–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ–∏—Å–∫–∞ —Å —É—Ç–æ—á–Ω—è—é—â–∏–º –≤–æ–ø—Ä–æ—Å–æ–º ===

def _get_articles_for_lemma_node(lemma_node: str, G: nx.MultiDiGraph) -> set:
    """–°–æ–±–∏—Ä–∞–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–æ article_id –∏–∑ –≤—Ö–æ–¥—è—â–∏—Ö instance_of —Ä—ë–±–µ—Ä (INST ‚Üí LEM)."""
    articles = set()
    for u, _, data in G.in_edges(lemma_node, data=True):
        if data.get("type") == "instance_of":
            aid = G.nodes[u].get("article_id")
            if aid is not None:
                articles.add(str(aid))
    return articles


def get_most_similar_lemma(word: str, G: nx.MultiDiGraph, nlp_model) -> str | None:
    """
    –ò—â–µ—Ç –ª–µ–º–º—É –≤ –≥—Ä–∞—Ñ–µ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º –∫–æ—Å–∏–Ω—É—Å–Ω—ã–º —Å—Ö–æ–¥—Å—Ç–≤–æ–º –ø–æ –≤–µ–∫—Ç–æ—Ä–∞–º spaCy.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç ID —É–∑–ª–∞-–ª–µ–º–º—ã, –µ—Å–ª–∏ —Å—Ö–æ–¥—Å—Ç–≤–æ > 0.55.
    """
    try:
        target_vec = nlp_model(word)
        if target_vec.vector_norm == 0:
            return None
    except Exception:
        return None

    best_sim = -1.0
    best_node = None

    for node, data in G.nodes(data=True):
        if data.get("type") != "lemma":
            continue
        lemma = data.get("lemma", "").strip()
        if not lemma:
            continue
        try:
            sim = nlp_model(lemma).similarity(target_vec)
        except Exception:
            continue
        if sim > best_sim:
            best_sim = sim
            best_node = node

    if best_node and best_sim > 0.55:
        print(f"–ü–æ—Ö–æ–∂–µ–µ —Å–ª–æ–≤–æ –¥–ª—è ¬´{word}¬ª ‚Üí ¬´{G.nodes[best_node]['lemma']}¬ª ({best_sim:.2f})")
        return best_node
    return None


def find_best_articles_for_question(question: str,
                                   G: nx.MultiDiGraph,
                                   nlp_model,
                                   normalize=True,
                                   df: pd.DataFrame = None) -> list:
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ–∏—Å–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –ø–æ –≤–æ–ø—Ä–æ—Å—É:
      ‚Ä¢ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –ª–µ–º–º—ã –≤–æ–ø—Ä–æ—Å–∞
      ‚Ä¢ —Ç–æ—á–Ω—ã–π –ø–æ–∏—Å–∫ + fallback –ø–æ –≤–µ–∫—Ç–æ—Ä–Ω–æ–º—É —Å—Ö–æ–¥—Å—Ç–≤—É
      ‚Ä¢ DFS –ø–æ lemma_dep —Å —É—á—ë—Ç–æ–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è —Å—Ç–∞—Ç–µ–π
      ‚Ä¢ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç top-3 —Å—Ç–∞—Ç–µ–π
      ‚Ä¢ –ø—Ä–∏ –±–ª–∏–∑–∫–∏—Ö score –∑–∞–¥–∞—ë—Ç —É—Ç–æ—á–Ω—è—é—â–∏–π –≤–æ–ø—Ä–æ—Å
    """
    doc = nlp_model(question)
    lemmas = [tok.lemma_.lower() for tok in doc if tok.pos_ not in EXCLUDED_POS]
    lemmas = list(dict.fromkeys(lemmas))  # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Ä—è–¥–æ–∫, —É–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏

    print(f"\n–í–æ–ø—Ä–æ—Å: {question}")
    print(f"–õ–µ–º–º—ã –≤–æ–ø—Ä–æ—Å–∞: {lemmas}")

    article_scores: dict[str, float] = {}

    for lemma in lemmas:
        # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        candidates = [
            n for n, d in G.nodes(data=True)
            if d.get("type") == "lemma" and str(d.get("lemma", "")) == lemma
        ]

        # –ü–æ—Ö–æ–∂–µ–µ —Å–ª–æ–≤–æ, –µ—Å–ª–∏ —Ç–æ—á–Ω–æ–≥–æ –Ω–µ—Ç
        if not candidates:
            sim_node = get_most_similar_lemma(lemma, G, nlp_model)
            if sim_node:
                candidates = [sim_node]

        for node in candidates:
            base_articles = _get_articles_for_lemma_node(node, G)
            if not base_articles:
                continue

            idf = LEMMA_IDF.get(G.nodes[node].get("lemma"), 1.0)
            for aid in base_articles:
                article_scores[aid] = article_scores.get(aid, 0.0) + 2.0 * idf

            # DFS –ø–æ lemma_dep (–±–µ–∑ –∂—ë—Å—Ç–∫–æ–≥–æ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –≥–ª—É–±–∏–Ω—ã)
            decay_factor = 0.7  # –ø–æ–¥–±–∏—Ä–∞–π (0.6-0.85)
            stack = [(node, set(base_articles), 0)]  # (node, articles, depth)
            visited = set()

            max_visited = 5000  # safety limit

            while stack:
                cur_node, cur_articles, depth = stack.pop()
                if cur_node in visited:
                    continue
                visited.add(cur_node)
                if len(visited) > max_visited:
                    break

                for _, neigh, edata in G.out_edges(cur_node, data=True):
                    if edata.get("type") != "lemma_dep":
                        continue

                    lu = G.nodes[cur_node].get("lemma")
                    lv = G.nodes[neigh].get("lemma")
                    pmi_weight = LEMMA_PMI.get((lu, lv), 1.0)

                    neigh_articles = _get_articles_for_lemma_node(neigh, G)
                    if not neigh_articles:
                        continue

                    # –¢—Ä–µ–±—É–µ–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —Å —Ç–µ–∫—É—â–µ–π –≤–µ—Ç–∫–æ–π
                    if not (neigh_articles & cur_articles):
                        continue

                    merged = neigh_articles | cur_articles
                    # —É—á—Ç—ë–º –∑–∞—Ç—É—Ö–∞–Ω–∏–µ –ø–æ –≥–ª—É–±–∏–Ω–µ
                    step_weight = (decay_factor ** depth) * pmi_weight
                    for aid in merged:
                        article_scores[aid] = article_scores.get(aid, 0.0) + 1.0 * step_weight

                    # –¥–æ–±–∞–≤–ª—è–µ–º —Å –≥–ª—É–±–∏–Ω–æ–π+1
                    stack.append((neigh, merged, depth + 1))


    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –ª–µ–º–º –≤–æ–ø—Ä–æ—Å–∞
    if normalize and lemmas:
        for aid in article_scores:
            article_scores[aid] /= len(lemmas)

    sorted_articles = sorted(article_scores.items(), key=lambda x: x[1], reverse=True)
    top_n = sorted_articles[:3]

    if not top_n:
        print("–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π.")
        return []

    print("–¢–æ–ø-–∫–∞–Ω–¥–∏–¥–∞—Ç—ã (aid, score):", top_n)

    # –£—Ç–æ—á–Ω—è—é—â–∏–π –≤–æ–ø—Ä–æ—Å –ø—Ä–∏ –±–ª–∏–∑–∫–∏—Ö score
    if len(top_n) >= 2:
        best, second = top_n[0][1], top_n[1][1]
        eps = 1e-9
        if (abs(best - second) / (abs(best) + eps)) < 0.10:
            print("–°–∏—Å—Ç–µ–º–∞ –Ω–µ —É–≤–µ—Ä–µ–Ω–∞ –º–µ–∂–¥—É –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ —Å—Ç–∞—Ç—å—è–º–∏ ‚Äî –Ω—É–∂–µ–Ω —É—Ç–æ—á–Ω—è—é—â–∏–π –≤–æ–ø—Ä–æ—Å.")

            if df is not None:
                top_ids = [aid for aid, _ in top_n[:2]]
                texts = {}
                for aid in top_ids:
                    rows = df[df["aid"] == aid]["value"].tolist()
                    if rows:
                        texts[aid] = " ".join(rows)

                if len(texts) == 2:
                    aid1, aid2 = list(texts.keys())
                    text1, text2 = texts[aid1], texts[aid2]

                    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ñ—Ä–∞–∑ (ADJ+NOUN –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ NOUN)
                    def extract_phrases(txt):
                        doc_ph = nlp_model(txt)
                        phrases = set()
                        for i, t in enumerate(doc_ph):
                            if t.pos_ == "ADJ" and i + 1 < len(doc_ph) and doc_ph[i + 1].pos_ == "NOUN":
                                phrases.add(f"{t.lemma_.lower()} {doc_ph[i+1].lemma_.lower()}")
                            elif t.pos_ == "NOUN" and len(t.lemma_) > 3:
                                phrases.add(t.lemma_.lower())
                        return phrases

                    ph1, ph2 = extract_phrases(text1), extract_phrases(text2)

                    # –ö–æ—Ä–Ω–∏ —Ç–µ–º–∞—Ç–∏–∫–∏
                    roots = {"–ø–æ—Å–æ–±–∏–µ", "–≤—ã–ø–ª–∞—Ç–∞", "—Ä–µ–±—ë–Ω–æ–∫", "—Å–µ–º—å—è", "—á–µ—Ä–Ω–æ–≤–∏–∫", "—à–∫–æ–ª–∞", "—Ä–æ–¥—Å—Ç–≤–æ"}
                    common = {r for r in roots if any(r in p for p in ph1) and any(r in p for p in ph2)}

                    # –ü–æ–∏—Å–∫ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ–π –ø–∞—Ä—ã –≤ –æ–±—â–µ–π —Ç–µ–º–µ
                    def contrast_pair(p1, p2):
                        best = None
                        best_sim = 0
                        for a in p1:
                            for b in p2:
                                if any(r in a and r in b for r in common):
                                    try:
                                        sim = nlp_model(a).similarity(nlp_model(b))
                                    except Exception:
                                        continue
                                    if 0.2 < sim < 0.7 and sim > best_sim:
                                        best_sim = sim
                                        best = (a, b)
                        return best

                    pair = contrast_pair(ph1, ph2)
                    if pair:
                        a, b = pair
                        print(f"–£—Ç–æ—á–Ω–µ–Ω–∏–µ: —Ä–µ—á—å –æ ¬´{a}¬ª –∏–ª–∏ –æ ¬´{b}¬ª?")
                    else:
                        # fallback –Ω–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∏
                        titles = []
                        for aid in [aid1, aid2]:
                            row = df[df["aid"] == aid]["value"].iloc[0]
                            short = next((r for r in df[df["aid"] == aid]["value"] if len(r.split()) < 10), row)
                            titles.append(short.strip()[:70])
                        print(f"–£—Ç–æ—á–Ω–µ–Ω–∏–µ: –≤—ã –∏–º–µ–ª–∏ –≤ –≤–∏–¥—É ¬´{titles[0]}¬ª –∏–ª–∏ ¬´{titles[1]}¬ª?")

    return top_n

from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
DFQ_PATH = "dfq.tsv"
def evaluate_question_search(G: nx.MultiDiGraph,
                             df: pd.DataFrame,
                             nlp_model,
                             dfq_path: str = DFQ_PATH):
    """
    –û—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –ø–æ–∏—Å–∫–∞ –ø–æ —Ç–µ—Å—Ç–æ–≤–æ–º—É –Ω–∞–±–æ—Ä—É dfq.tsv.
    –í—ã–≤–æ–¥–∏—Ç —Ç–æ–ø-3 —Å—Ç–∞—Ç—å–∏ —Å –∑–∞–≥–æ–ª–æ–≤–∫–æ–º –∏ —Å–Ω–∏–ø–ø–µ—Ç–æ–º.
    –°—á–∏—Ç–∞–µ—Ç Accuracy@1, Accuracy@3, Precision, Recall, F1-score, Loss.
    """
    if not os.path.exists(dfq_path):
        print(f"–§–∞–π–ª {dfq_path} –Ω–µ –Ω–∞–π–¥–µ–Ω.")
        return

    dfq = pd.read_csv(dfq_path, sep="\t", dtype=str).fillna("")
    print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(dfq)} —Ç–µ—Å—Ç–æ–≤ –∏–∑ {dfq_path}")
    print("–ö–æ–ª–æ–Ω–∫–∏:", list(dfq.columns))

    # –ü–æ–∏—Å–∫ –∫–æ–ª–æ–Ω–æ–∫ –ø–æ –∞–ª–∏–∞—Å–∞–º
    q_cols = ["question", "q", "query", "user_qu", "–≤–æ–ø—Ä–æ—Å"]
    exp_cols = ["expected", "true", "aid_true", "aid", "aid_expected", "answer"]

    q_col = next((c for c in dfq.columns if c.lower() in q_cols), None)
    exp_col = next((c for c in dfq.columns if c.lower() in exp_cols), None)

    if not q_col or not exp_col:
        print("–ù–µ –Ω–∞–π–¥–µ–Ω—ã –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ (–≤–æ–ø—Ä–æ—Å / –æ–∂–∏–¥–∞–µ–º—ã–π aid).")
        return

    print(f"–ö–æ–ª–æ–Ω–∫–∏: question='{q_col}', expected='{exp_col}'")

    df["aid"] = df["aid"].astype(str)
    acc1 = acc3 = total = 0
    y_true = []
    y_pred_top1 = []
    y_pred_top3 = []

    for idx, row in dfq.iterrows():
        question = str(row[q_col]).strip()
        expected = str(row[exp_col]).strip()
        if not question or not expected:
            continue

        print(f"\n=== –¢–µ—Å—Ç {idx + 1} ===")
        print(f"–í–æ–ø—Ä–æ—Å: {question}")
        print(f"–û–∂–∏–¥–∞–µ—Ç—Å—è: {expected}")

        results = find_best_articles_for_question(
            question, G, nlp_model, df=df, normalize=True
        )

        total += 1
        print("\n–¢–æ–ø-3 –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π:")

        found_aids = []
        for rank, (aid, score) in enumerate(results[:3], 1):
            aid_str = str(aid)
            found_aids.append(aid_str)

            rows = df[df["aid"] == aid_str]
            if rows.empty:
                print(f"{rank}. aid={aid_str} | score={score:.3f} ‚Üí —Å—Ç–∞—Ç—å—è –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ df.tsv")
                continue

            text = " ".join(rows["value"].tolist())
            snippet = text[:350] + "‚Ä¶" if len(text) > 350 else text
            header = next((v for v in rows["value"] if len(v.split()) < 12), text.split(".")[0])

            print(f"{rank}. aid={aid_str} | score={score:.3f}")
            print(f"   –ó–∞–≥–æ–ª–æ–≤–æ–∫: {header}")
            print(f"   –°–Ω–∏–ø–ø–µ—Ç: {snippet}\n")

        # === –ú–µ—Ç—Ä–∏–∫–∏ —Å —É—á—ë—Ç–æ–º —Ä–∞–≤–µ–Ω—Å—Ç–≤–∞ —Å–∫–æ—Ä–∏–Ω–≥–æ–≤ ===
        if found_aids:
            top_score = results[0][1]
            tied_top = [aid for aid, sc in results if abs(sc - top_score) < 1e-9]

            # Accuracy@1
            acc1_flag = int(expected in tied_top)
            acc1 += acc1_flag

            # Accuracy@3
            if len(results) >= 3:
                third_score = results[2][1]
                tied_top3 = [aid for aid, sc in results if sc >= third_score - 1e-9]
                acc3_flag = int(expected in tied_top3)
                acc3 += acc3_flag
            else:
                last_score = results[-1][1]
                tied_topN = [aid for aid, sc in results if sc >= last_score - 1e-9]
                acc3_flag = int(expected in tied_topN)
                acc3 += acc3_flag

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è –º–µ—Ç—Ä–∏–∫
            y_true.append(1)
            y_pred_top1.append(acc1_flag)
            y_pred_top3.append(acc3_flag)

        print("-" * 100)

    # === –ò—Ç–æ–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ ===
    if total > 0:
        accuracy1 = acc1 / total
        accuracy3 = acc3 / total
        loss = 1 - accuracy1

        precision = precision_score(y_true, y_pred_top1, zero_division=0)
        recall = recall_score(y_true, y_pred_top1, zero_division=0)
        f1 = f1_score(y_true, y_pred_top1, zero_division=0)

        print("\n=== üìä –ò—Ç–æ–≥–∏ ===")
        print(f"–í—Å–µ–≥–æ —Ç–µ—Å—Ç–æ–≤: {total}")
        print(f"Accuracy@1: {accuracy1:.3f} ({acc1}/{total})")
        print(f"Accuracy@3: {accuracy3:.3f} ({acc3}/{total})")
        print(f"Loss:       {loss:.3f}")
        print(f"Precision:  {precision:.3f}")
        print(f"Recall:     {recall:.3f}")
        print(f"F1-score:   {f1:.3f}")
    else:
        print("–ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤.")

# === –ó–∞–ø—É—Å–∫ –æ—Ü–µ–Ω–∫–∏ ===
evaluate_question_search(G, df, nlp, DFQ_PATH)

# === –ë–ª–æ–∫: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –≥—Ä–∞—Ñ–∞ (–≤—Å–µ —É–∑–ª—ã –∏ —Å–≤—è–∑–∏) ===
triplets = []

# –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –≥—Ä–∞—Ñ G (–Ω–µ–æ—á–∏—â–µ–Ω–Ω—ã–π)
for u, v, data in G.edges(data=True):
    triplets.append({
        "source": u,
        "target": v,
        "relation": data.get("label") or data.get("type") or "",
        "type": data.get("type", ""),
        "attributes": json.dumps(data, ensure_ascii=False)
    })

# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ DataFrame
triplet_df = pd.DataFrame(triplets)

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–¥ –∏–º–µ–Ω–∞–º–∏, —è–≤–Ω–æ –æ–±–æ–∑–Ω–∞—á–∞—é—â–∏–º–∏, —á—Ç–æ —ç—Ç–æ –ø–æ–ª–Ω—ã–π –≥—Ä–∞—Ñ
csv_path = os.path.join(OUT_DIR, "syntactic_graph_triplets_raw.csv")
tsv_path = os.path.join(OUT_DIR, "syntactic_graph_triplets_raw.tsv")

triplet_df.to_csv(csv_path, index=False, encoding="utf-8")
triplet_df.to_csv(tsv_path, index=False, sep="\t", encoding="utf-8")

print(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(triplet_df)} —Ç—Ä–∏–ø–ª–µ—Ç–æ–≤ (–∏—Å—Ö–æ–¥–Ω—ã–π –≥—Ä–∞—Ñ):")
print(f"  CSV: {csv_path}")
print(f"  TSV: {tsv_path}")