# -*- coding: utf-8 -*-
"""HybridGraph.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/185sk7qCKuAH5HtBUizI88R3SmixYOqP0
"""

!pip install sentence-transformers spacy networkx tqdm pandas pyvis transformers
!python -m spacy download ru_core_news_sm

import os, re, json, pickle, math, shutil
from collections import defaultdict, Counter
from typing import List, Tuple
import numpy as np
import pandas as pd
import networkx as nx
from tqdm import tqdm
from sentence_transformers import SentenceTransformer
import spacy
from pyvis.network import Network

# --- –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ ---

DF_PATH = "df.tsv"
DFQ_PATH = "dfq.tsv"
CACHE_PATH = "graph_hybrid_cache.pkl"
EVAL_OUT = "hybrid_graph_eval.csv"
HTML_OUT = "graph_hybrid.html"
MODEL_NAME = "deepvk/USER-base" # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å –∏–∑ –∏—Å—Ö–æ–¥–Ω—ã—Ö –∫–æ–¥–æ–≤
TOP_K_NEIGHBORS = 5

# --- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –∏ spaCy ---

try:
    model = SentenceTransformer(MODEL_NAME)
    VECTOR_DIM = model.get_sentence_embedding_dimension()
    print(f"BERT loaded: {MODEL_NAME} dim={VECTOR_DIM}")
except Exception as e:
    print(f"Error loading SentenceTransformer: {e}")
    exit()

try:
    nlp = spacy.load("ru_core_news_sm")
    SPACY_OK = True
    print("spaCy Russian model loaded (ru_core_news_sm).")
except Exception:
    nlp = None
    SPACY_OK = False
    print("spaCy ru_core_news_sm not available. Syntactic features will be disabled.")

# --- –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ---

def encode_batch(texts: List[str]) -> np.ndarray:
    if not texts:
        return np.zeros((0, VECTOR_DIM), dtype=np.float32)
    embs = model.encode(texts, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=False)
    return embs.astype(np.float32)

def cos_sim(a: np.ndarray, b: np.ndarray) -> float:
    return float(np.dot(a, b))

def normalize_token(tok: str) -> str:
    return re.sub(r"[^–∞-—è—ëa-z0-9\-]", "", tok.lower())

def extract_keywords_simple(text: str, min_len: int = 4, top_n: int = 8) -> List[str]:
    toks = re.findall(r"\w+", text.lower(), flags=re.UNICODE)
    toks = [normalize_token(t) for t in toks if len(t) >= min_len]
    seen = set()
    out = []
    for t in toks:
        if not t or t in seen:
            continue
        seen.add(t)
        out.append(t)
        if len(out) >= top_n:
            break
    return out

def extract_triplets_spacy(text: str) -> List[Tuple[str,str,str]]:
    if not SPACY_OK:
        return []
    doc = nlp(text)
    triplets = []
    for token in doc:
        if token.dep_.lower() in ("obj", "iobj", "dobj", "pobj") and token.head:
            subj = None
            for child in token.head.lefts:
                if child.dep_.lower().startswith("nsubj"):
                    subj = child.text
                    break
            if subj:
                triplets.append((normalize_token(subj), normalize_token(token.head.text), normalize_token(token.text)))
    seen = set(); out=[]
    for t in triplets:
        if t not in seen:
            seen.add(t); out.append(t)
    return out

def extract_triplets_fallback(text: str) -> List[Tuple[str,str,str]]:
    parts = re.split(r'[.!?]', text)
    out=[]
    for s in parts:
        words = re.findall(r'\w+', s)
        if len(words) >= 3:
            out.append((normalize_token(words[0]), normalize_token(words[1]), normalize_token(" ".join(words[2:5]))))
        if len(out) >= 3:
            break
    return out

def extract_triplets(text: str) -> List[Tuple[str,str,str]]:
    res = extract_triplets_spacy(text)
    if res:
        return res
    return extract_triplets_fallback(text)

# --- –ë–ª–æ–∫ 4: –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ì–ò–ë–†–ò–î–ù–û–ì–û –≥—Ä–∞—Ñ–∞ ---

def build_graph(rebuild: bool = False):
    if os.path.exists(CACHE_PATH) and not rebuild:
        try:
            with open(CACHE_PATH, "rb") as f:
                data = pickle.load(f)
            G, article_texts, VECTOR_DIM_CACHED = data
            if VECTOR_DIM_CACHED == VECTOR_DIM:
                print("Loaded hybrid graph from cache.")
                return G, article_texts
            else:
                print("Vector dim mismatch ‚Äî rebuilding graph.")
        except Exception as e:
            print("Cache load failed:", e)

    df = pd.read_csv(DF_PATH, sep="\t", dtype={"aid": str})
    assert {'aid', 'type', 'value'} <= set(df.columns), "df.tsv must contain columns 'aid','type','value'"

    print("Building HYBRID Semantic-Syntactic knowledge graph...")
    G = nx.DiGraph() # –ò—Å–ø–æ–ª—å–∑—É–µ–º DiGraph –¥–ª—è –∏–µ—Ä–∞—Ä—Ö–∏–∏ –∏ —Å–∏–Ω—Ç–∞–∫—Å–∏—Å–∞
    article_texts = {}
    all_entities = set()
    entity_vectors = {}

    for aid, group in tqdm(df.groupby("aid"), desc="Articles", total=len(df["aid"].unique())):
        art_node = f"art_{aid}"
        header_rows = group[group["type"] == "header"]
        title = header_rows["value"].iloc[0] if not header_rows.empty else f"–°—Ç–∞—Ç—å—è {aid}"
        art_vec = encode_batch([str(title)])[0]
        G.add_node(art_node, type="article", aid=aid, title=str(title), vector=art_vec)

        frag_texts = []
        frag_index = 0

        for _, row in group.iterrows():
            frag_text = str(row.value).strip()
            if not frag_text:
                continue

            frag_node = f"frag_{aid}_{frag_index}"
            frag_vec = encode_batch([frag_text])[0]
            G.add_node(frag_node, type="fragment", text=frag_text, vector=frag_vec)
            G.add_edge(art_node, frag_node, relation="contains")
            frag_texts.append(frag_text)
            frag_index += 1

            # --- 1. –°–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∞—è –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏—è (–∏–∑ syntaksisgraf) ---
            if SPACY_OK:
                doc = nlp(frag_text)
                for sent_idx, sent in enumerate(doc.sents):
                    sent_text = sent.text.strip()
                    if len(sent_text) < 10:
                        continue

                    sent_node = f"sent_{aid}_{frag_index}_{sent_idx}"
                    sent_vec = encode_batch([sent_text])[0]
                    G.add_node(sent_node, type="sentence", text=sent_text, vector=sent_vec)
                    G.add_edge(frag_node, sent_node, relation="has_sentence")

                    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ (–¥–ª—è —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞)
                    for token in sent:
                        if token.pos_ in ("PUNCT", "SPACE") or token.is_stop:
                            continue
                        tok_node = f"tok_{aid}_{frag_index}_{sent_idx}_{token.i}"
                        tok_vec = encode_batch([token.lemma_])[0]
                        G.add_node(tok_node, type="token", text=token.text, lemma=token.lemma_, pos=token.pos_, dep=token.dep_, vector=tok_vec)
                        G.add_edge(sent_node, tok_node, relation="has_token")

                        # –ü–æ–º–µ—Ç–∫–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏—Ö —Ä–æ–ª–µ–π (–¥–ª—è –ø–æ–∏—Å–∫–∞)
                        if token == sent.root:
                            G.add_edge(sent_node, tok_node, relation="root")
                        if token.dep_ in ("nsubj", "nsubj:pass"):
                            G.add_edge(sent_node, tok_node, relation="subject")
                        elif token.dep_ in ("obj", "iobj", "dobj"):
                            G.add_edge(sent_node, tok_node, relation="object")

            # --- 2. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Ç—Ä–∏–ø–ª–µ—Ç—ã –∏ —Å—É—â–Ω–æ—Å—Ç–∏ (–∏–∑ semanticgraf) ---
            trips = extract_triplets(frag_text)
            for t_i, (subj, rel, obj) in enumerate(trips):
                if not subj or not obj or not rel:
                    continue
                subj = subj.strip(); rel = rel.strip(); obj = obj.strip()
                if not subj or not obj or not rel:
                    continue

                all_entities.update([subj, obj])

                tri_node = f"tri_{aid}_{frag_index}_{t_i}"
                tri_text = f"{subj} {rel} {obj}"
                tri_vec = encode_batch([tri_text])[0]
                G.add_node(tri_node, type='triple', text=tri_text, vector=tri_vec)
                G.add_edge(frag_node, tri_node, relation='has_triplet')

                # –î–æ–±–∞–≤–ª—è–µ–º —É–∑–ª—ã —Å—É—â–Ω–æ—Å—Ç–µ–π –∏ —Å–≤—è–∑–∏
                G.add_node(subj, type='entity', name=subj)
                G.add_node(obj, type='entity', name=obj)
                G.add_edge(tri_node, subj, relation='subject_of', label=rel)
                G.add_edge(tri_node, obj, relation='object_of', label=rel)

        article_text = " ".join(frag_texts)
        article_texts[aid] = article_text
        G.nodes[art_node]["vector"] = encode_batch([article_text])[0]

    # --- 3. –°–ª–∏—è–Ω–∏–µ —Å–∏–Ω–æ–Ω–∏–º–∏—á–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π (–∏–∑ semanticgraf) ---
    print("Encoding entities for semantic merging...")
    entity_list = list(all_entities)
    if entity_list:
        entity_vecs = encode_batch(entity_list)
        for e, v in zip(entity_list, entity_vecs):
            entity_vectors[e] = v

    print("Merging semantically similar entities...")
    merged_map = {}
    used = set()
    SIM_THRESHOLD = 0.85 # –ü–æ—Ä–æ–≥ –∏–∑ semanticgraf
    for i, e1 in enumerate(entity_list):
        if e1 in used:
            continue
        cluster = [e1]
        v1 = entity_vectors.get(e1)
        if v1 is None or not np.any(v1):
            continue
        for j, e2 in enumerate(entity_list):
            if i == j or e2 in used:
                continue
            v2 = entity_vectors.get(e2)
            if v2 is None or not np.any(v2):
                continue
            sim = np.dot(v1, v2)
            if sim > SIM_THRESHOLD:
                cluster.append(e2)
                used.add(e2)
        main = cluster[0]
        for c in cluster:
            merged_map[c] = main

    print("Updating entity references after merging...")
    for e, canonical in merged_map.items():
        if e == canonical:
            continue
        if e in G.nodes and canonical in G.nodes:
            for n in list(G.neighbors(e)):
                data = G.get_edge_data(e, n)
                if not G.has_edge(canonical, n):
                    G.add_edge(canonical, n, **data)
            G.remove_node(e)
        elif e in G.nodes and canonical not in G.nodes:
            G.nodes[e]['name'] = canonical

    # --- 4. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–±—Ä–∞ –º–µ–∂–¥—É –±–ª–∏–∑–∫–∏–º–∏ —Ç—Ä–∏–ø–ª–µ—Ç–∞–º–∏ (–∏–∑ semanticgraf) ---
    print("Computing semantic neighbor edges (triplets)...")
    tri_nodes = [n for n, d in G.nodes(data=True) if d.get('type') == 'triple']
    tri_vecs = np.vstack([G.nodes[n]['vector'] for n in tri_nodes]) if tri_nodes else np.zeros((0, VECTOR_DIM))
    TRIPLET_SIM_THRESHOLD = 0.45 # –ü–æ—Ä–æ–≥ –∏–∑ semanticgraf
    for i, n1 in enumerate(tri_nodes):
        v = tri_vecs[i]
        sims = tri_vecs @ v
        sims[i] = -1
        top_idx = np.argpartition(-sims, min(5, len(sims)-1))[:5]
        for j in top_idx:
            if sims[j] > TRIPLET_SIM_THRESHOLD:
                n2 = tri_nodes[j]
                if not G.has_edge(n1, n2):
                    sim = float(sims[j])
                    G.add_edge(n1, n2, relation='semantic_triple', sim=sim, w=max(0.0001, 1.0 - sim))

    # --- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫—ç—à ---
    with open(CACHE_PATH, "wb") as f:
        pickle.dump((G, article_texts, VECTOR_DIM), f)

    print("HYBRID knowledge graph built and cached.")
    return G, article_texts

G, article_texts = build_graph(rebuild=True)

# --- –ë–ª–æ–∫ 5: –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ ---

def shortest_path_w_distance(G: nx.Graph, source: str, target: str, cutoff: int = 6) -> float:
    try:
        return float(nx.shortest_path_length(G, source=source, target=target, weight='w', cutoff=cutoff))
    except Exception:
        return float('inf')

def search_graph_hybrid(question: str, G: nx.DiGraph, article_texts: dict, top_k: int = 3):
    q_doc = nlp(question) if SPACY_OK else None
    q_vec = encode_batch([question])[0]
    q_kws = set(extract_keywords_simple(question, min_len=3, top_n=12))

    # --- 1. –ê–Ω–∞–ª–∏–∑ –≤–æ–ø—Ä–æ—Å–∞ –¥–ª—è —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–æ–≥–æ —Å–∫–æ—Ä–∏–Ω–≥–∞ (–∏–∑ syntaksisgraf) ---
    q_root, q_subj, q_obj = None, None, None
    if q_doc:
        q_sents = list(q_doc.sents)
        if q_sents:
            q_sent = q_sents[0]
            q_root = q_sent.root
            for child in q_root.children:
                if child.dep_ in ("nsubj", "nsubj:pass"):
                    q_subj = child
                elif child.dep_ in ("obj", "iobj", "dobj"):
                    q_obj = child

    # --- 2. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å—Ç–∞—Ç–µ–π, —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤, —Ç—Ä–∏–ø–ª–µ—Ç–æ–≤, —Å—É—â–Ω–æ—Å—Ç–µ–π (–∏–∑ semanticgraf) ---
    article_nodes = [n for n, d in G.nodes(data=True) if d.get("type") == "article"]
    article_kw_cache = {
        n: set(extract_keywords_simple(article_texts.get(G.nodes[n]["aid"], ""), min_len=3, top_n=20))
        for n in article_nodes
    }
    art_scores = {
        n: cos_sim(G.nodes[n].get("vector", np.zeros(VECTOR_DIM)), q_vec)
        for n in article_nodes
    }

    def get_top_nodes(node_type: str, limit: int = 50):
        nodes = [n for n, d in G.nodes(data=True) if d.get("type") == node_type and "vector" in d]
        if not nodes: return [], np.array([])
        vecs = np.vstack([G.nodes[n]["vector"] for n in nodes])
        sims = vecs @ q_vec
        top_idx = np.argpartition(-sims, min(limit, len(sims)-1))[:min(limit, len(sims))]
        return [nodes[i] for i in top_idx], sims

    top_frags, frag_sims = get_top_nodes("fragment")
    top_tris, tri_sims = get_top_nodes("triple")
    top_ents, ent_sims = get_top_nodes("entity")

    # --- 3. –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ (–∏–∑ semanticgraf) ---
    candidate_articles = set()
    for a, _ in sorted(art_scores.items(), key=lambda x: x[1], reverse=True)[:max(10, top_k * 5)]:
        candidate_articles.add(a)
    for nset in [top_frags[:30], top_tris[:30], top_ents[:30]]:
        for n in nset:
            for nbr in G.neighbors(n):
                if G.nodes[nbr].get("type") == "article":
                    candidate_articles.add(nbr)

    # --- 4. –û—Ü–µ–Ω–∫–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ (–ì–∏–±—Ä–∏–¥–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞) ---
    results = []
    SEMANTIC_RELS = {"subject_of", "object_of", "semantic_triple"}
    entities_in_question = set(
        e for e, d in G.nodes(data=True)
        if d.get("type") == "entity" and e.lower() in question.lower()
    )

    for art in candidate_articles:
        direct = art_scores.get(art, 0.0)

        # 4.1. –í–∫–ª–∞–¥ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ (Semanticgraf)
        frag_sims_for_art = [
            cos_sim(G.nodes[f]["vector"], q_vec)
            for f in G.predecessors(art)
            if G.nodes[f].get("type") == "fragment"
        ]
        frag_contrib = np.mean(sorted(frag_sims_for_art, reverse=True)[:5]) if frag_sims_for_art else 0.0

        # 4.2. –í–∫–ª–∞–¥ –ø—É—Ç–∏ (Semanticgraf)
        best_path_score = 0.0
        for src_set, sims, w in [(top_tris, tri_sims, 1.3), (top_frags, frag_sims, 1.0), (top_ents, ent_sims, 1.1)]:
            for src in src_set[:25]:
                path_len = shortest_path_w_distance(G, src, art, cutoff=6)
                if path_len != float("inf"):
                    path_score = 1.0 / (1.0 + path_len)
                    src_sim = float(np.dot(G.nodes[src]["vector"], q_vec))
                    score = path_score * max(1.0, src_sim * w)
                    if score > best_path_score:
                        best_path_score = score

                # –û–±—Ö–æ–¥ —á–µ—Ä–µ–∑ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Å–≤—è–∑–∏
                for nbr in G.successors(src):
                    rel = G.get_edge_data(src, nbr).get("relation")
                    if rel in SEMANTIC_RELS:
                        path_len_sem = shortest_path_w_distance(G, nbr, art, cutoff=6)
                        if path_len_sem != float("inf"):
                            path_score = 1.0 / (1.0 + path_len_sem)
                            src_sim = float(np.dot(G.nodes[src]["vector"], q_vec))
                            score = 1.15 * path_score * max(1.0, src_sim * w)
                            if score > best_path_score:
                                best_path_score = score

        # 4.3. –°–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏–π –±–æ–Ω—É—Å (Syntaksisgraf - –ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω)
        syn_bonus = 0.0
        if SPACY_OK and q_root:
            # –ù–∞—Ö–æ–¥–∏–º 5 —Å–∞–º—ã—Ö —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –±–ª–∏–∑–∫–∏—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ —Å—Ç–∞—Ç—å–µ
            art_sentences = [n for n in nx.descendants(G, art) if G.nodes[n].get("type") == "sentence"]
            if art_sentences:
                art_sent_vecs = np.vstack([G.nodes[n]["vector"] for n in art_sentences])
                sent_sims = art_sent_vecs @ q_vec
                top_sent_idx = np.argpartition(-sent_sims, min(5, len(sent_sims)-1))[:min(5, len(sent_sims))]

                for i in top_sent_idx:
                    sent_node = art_sentences[i]
                    sem_sim = float(sent_sims[i])
                    syn_score = 0.0

                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
                    sent_root_vec, sent_subj_vec, sent_obj_vec = None, None, None
                    for _, nbr, data in G.out_edges(sent_node, data=True):
                        rel = data.get("relation")
                        if rel == "root":
                            sent_root_vec = G.nodes[nbr]["vector"]
                        elif rel == "subject":
                            sent_subj_vec = G.nodes[nbr]["vector"]
                        elif rel == "object":
                            sent_obj_vec = G.nodes[nbr]["vector"]

                    # –í—ã—á–∏—Å–ª—è–µ–º —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
                    if q_root and sent_root_vec is not None:
                        q_root_vec = encode_batch([q_root.lemma_])[0]
                        syn_score += 0.4 * cos_sim(q_root_vec, sent_root_vec)
                    if q_subj and sent_subj_vec is not None:
                        q_subj_vec = encode_batch([q_subj.lemma_])[0]
                        syn_score += 0.3 * cos_sim(q_subj_vec, sent_subj_vec)
                    if q_obj and sent_obj_vec is not None:
                        q_obj_vec = encode_batch([q_obj.lemma_])[0]
                        syn_score += 0.3 * cos_sim(q_obj_vec, sent_obj_vec)

                    # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–∫–æ—Ä –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
                    combined_sent_score = 0.6 * sem_sim + 0.4 * syn_score
                    syn_bonus = max(syn_bonus, combined_sent_score)

        # 4.4. –ë–æ–Ω—É—Å –∑–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ (Semanticgraf)
        kw_overlap = len(article_kw_cache[art] & q_kws)
        kw_bonus = min(1.0, 0.15 * kw_overlap)

        # 4.5. –ë–æ–Ω—É—Å –∑–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π (Semanticgraf)
        ent_bonus = 0.0
        for e in entities_in_question:
            path_len_ent = shortest_path_w_distance(G, e, art, cutoff=5)
            if path_len_ent != float("inf"):
                ent_bonus += 0.05 * (1.0 / (1.0 + path_len_ent))
        ent_bonus = min(0.25, ent_bonus)

        # 4.6. –§–ò–ù–ê–õ–¨–ù–ê–Ø –ì–ò–ë–†–ò–î–ù–ê–Ø –û–¶–ï–ù–ö–ê
        final = (
            0.35 * direct +
            0.25 * frag_contrib +
            0.20 * best_path_score +
            0.10 * syn_bonus + # –ù–æ–≤—ã–π –≤–µ—Å –¥–ª—è —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–æ–≥–æ –≤–∫–ª–∞–¥–∞
            0.05 * kw_bonus +
            0.05 * ent_bonus
        )

        results.append((art, final, {
            "direct": direct,
            "frag": frag_contrib,
            "path": best_path_score,
            "syn_bonus": syn_bonus,
            "kw_bonus": kw_bonus,
            "ent_bonus": ent_bonus
        }))

    # --- 5. –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –∏ –≤—ã–≤–æ–¥ ---
    results.sort(key=lambda x: x[1], reverse=True)
    top_results = [{
        "aid": G.nodes[a]["aid"],
        "node": a,
        "title": G.nodes[a]["title"],
        "score": float(s),
        "meta": m
    } for a, s, m in results[:top_k]]

    print(f"\nQuery: {question!r}")
    if not top_results:
        print("   No results found.")
    else:
        for i, r in enumerate(top_results, 1):
            print(f"   {i}. AID={r['aid']} | {r['title'][:120]} | score={r['score']:.4f} | meta={r['meta']}")

    return top_results

# --- –ë–ª–æ–∫ 6: –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è (–ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –≥—Ä–∞—Ñ–∞) ---

def visualize_graph_hybrid(G, limit_nodes=300, save_path=HTML_OUT):
    print(f" –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ì–ò–ë–†–ò–î–ù–û–ì–û –≥—Ä–∞—Ñ–∞ (–¥–æ {limit_nodes} —É–∑–ª–æ–≤)...")
    net = Network(height="750px", width="100%", notebook=True, directed=True)
    net.set_options("""
    {
      "edges": {
        "smooth": false, "scaling": { "min": 0.1, "max": 0.1 },
        "color": { "color": "#888888", "highlight": "#555555", "hover": "#555555" },
        "width": 0.3
      },
      "nodes": {
        "scaling": { "min": 15, "max": 35 }, "font": { "size": 18 }
      },
      "physics": {
        "enabled": true,
        "barnesHut": {
          "gravitationalConstant": -8000, "springLength": 250,
          "springConstant": 0.05, "avoidOverlap": 0.2
        }
      },
      "interaction": { "hover": true, "tooltipDelay": 200 }
    }
    """)

    colors = {
        "article": "#ffcc00", "fragment": "#66ccff", "triple": "#ff9999",
        "entity": "#b3ffb3", "keyword": "#ffccff", "sentence": "#ff69b4", "token": "#98fb98"
    }

    nodes = list(G.nodes)[:limit_nodes]
    subG = G.subgraph(nodes).copy()

    for n, d in subG.nodes(data=True):
        ntype = d.get("type", "unknown")
        label = d.get("title", d.get("text", d.get("name", d.get("lemma", n))))[:80]
        color = colors.get(ntype, "#cccccc")
        net.add_node(
            n, label=label, color=color,
            title=f"{ntype}: {label} ({d.get('aid', '')})"
        )

    for u, v, data in subG.edges(data=True):
        if u in subG.nodes and v in subG.nodes:
            rel = data.get("relation", "—Å–≤—è–∑—å")
            sim = data.get("sim", 0.5)
            net.add_edge(
                u, v,
                label=rel,
                title=f"{rel} (sim={sim:.2f})",
                arrows="to"
            )

    net.show(save_path)
    print(f"‚úÖ –ì–∏–±—Ä–∏–¥–Ω—ã–π –≥—Ä–∞—Ñ —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤: {save_path}")

visualize_graph_hybrid(G, limit_nodes=300)

# --- –ë–ª–æ–∫ 7: –û—Å–Ω–æ–≤–Ω–æ–π –∑–∞–ø—É—Å–∫ –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ ---
if __name__ == "__main__":

      if os.path.exists(DFQ_PATH):
        dfq = pd.read_csv(DFQ_PATH, sep="\t")
        rows = []
        correct_1 = 0
        correct_3 = 0

        print("\nüîç –ó–∞–ø—É—Å–∫ –æ—Ü–µ–Ω–∫–∏ –ø–æ –ì–ò–ë–†–ò–î–ù–û–ú–£ –≥—Ä–∞—Ñ—É...\n")

        for _, row in tqdm(dfq.iterrows(), total=len(dfq), desc="Eval"):
            question = row["user_qu"]
            true_aid = str(int(row["aid"])) if not pd.isna(row["aid"]) else None

            res = search_graph_hybrid(question, G, article_texts, top_k=3)

            if not res:
                rows.append({"question": question, "true": true_aid, "pred_top1": None, "pred_top3": [], "ok@1": False, "ok@3": False})
                continue

            exp_text = article_texts.get(true_aid, "")[:400].replace("\n", " ")
            print(f"\n EXPECTED TEXT ({true_aid}): {exp_text}\n")

            pred = res[0]["aid"]
            top3_aids = [r["aid"] for r in res]

            ok1 = (pred == true_aid)
            ok3 = (true_aid in top3_aids)
            correct_1 += int(ok1)
            correct_3 += int(ok3)

            rows.append({
                "question": question,
                "true": true_aid,
                "pred_top1": pred,
                "pred_top3": top3_aids,
                "ok@1": ok1,
                "ok@3": ok3,
            })

        acc1 = (correct_1 / len(dfq) * 100) if len(dfq) > 0 else 0.0
        acc3 = (correct_3 / len(dfq) * 100) if len(dfq) > 0 else 0.0

        print(f"\n Accuracy@1 = {acc1:.2f}% ({correct_1}/{len(dfq)})")
        print(f" Accuracy@3 = {acc3:.2f}% ({correct_3}/{len(dfq)})")

        pd.DataFrame(rows).to_csv(EVAL_OUT, index=False, encoding="utf-8-sig")
        print("Detailed eval saved to", EVAL_OUT)