# -*- coding: utf-8 -*-
"""Graphormer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rCsMFIQUp9d4BUxMOTbrYlHAyEKZcuLW
"""

!pip install torch networkx numpy scikit-learn matplotlib pyvis
!pip install spacy && python -m spacy download en_core_web_sm

import re
import json
import numpy as np
import pandas as pd
from tqdm import tqdm
import networkx as nx
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import torch
import torch.nn as nn
import torch.nn.functional as F

# ----------------------------
# ÐŸÐ°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹
# ----------------------------
DF_PATH = "df.tsv"
DFQ_PATH = "dfq.tsv"
TOKEN_WINDOW = 3
EDGE_SIM_THRESHOLD = 0.6
DIST_MAX = 10
GRAPHORMER_D_MODEL = 128
GRAPHORMER_N_HEADS = 4

df = pd.read_csv(DF_PATH, sep="\t", dtype=str)
dfq = pd.read_csv(DFQ_PATH, sep="\t", dtype=str)

print("ðŸ“˜ df (Ð¿ÐµÑ€Ð²Ñ‹Ðµ ÑÑ‚Ñ€Ð¾ÐºÐ¸):")
print(df.head(), "\n")
print("ðŸ“— dfq (Ð¿ÐµÑ€Ð²Ñ‹Ðµ ÑÑ‚Ñ€Ð¾ÐºÐ¸):")
print(dfq.head(), "\n")

assert {"aid", "value"}.issubset(df.columns), "df Ð´Ð¾Ð»Ð¶ÐµÐ½ Ð¸Ð¼ÐµÑ‚ÑŒ ÐºÐ¾Ð»Ð¾Ð½ÐºÐ¸ aid, value"
assert "user_qu" in dfq.columns, "dfq Ð´Ð¾Ð»Ð¶ÐµÐ½ Ð¸Ð¼ÐµÑ‚ÑŒ ÐºÐ¾Ð»Ð¾Ð½ÐºÑƒ user_qu"

def tokenize(text):
    if not isinstance(text, str): text = ""
    toks = re.findall(r"[A-Za-zÐ-Ð¯Ð°-ÑÑ‘Ð0-9]+(?:-[A-Za-zÐ-Ð¯Ð°-ÑÑ‘Ð0-9]+)?", text)
    return [t.lower() for t in toks if len(t) > 0]

token_index, node_to_token, token_occurrences = {}, [], {}
aid_to_token_ids = {}

for _, row in df.iterrows():
    aid, text = row["aid"], row.get("value", "")
    toks = tokenize(text)
    if not toks: continue
    aid_to_token_ids.setdefault(aid, [])
    for t in toks:
        if t not in token_index:
            token_index[t] = len(node_to_token)
            node_to_token.append(t)
            token_occurrences[t] = set()
        token_occurrences[t].add(aid)
        aid_to_token_ids[aid].append(token_index[t])

# Ð´Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ñ‚Ð¾ÐºÐµÐ½Ñ‹ Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ¾Ð²
for q in dfq["user_qu"].astype(str):
    for t in tokenize(q):
        if t not in token_index:
            token_index[t] = len(node_to_token)
            node_to_token.append(t)
            token_occurrences[t] = set()

G = nx.Graph()
G.add_nodes_from(range(len(node_to_token)))
for nid, tok in enumerate(node_to_token):
    G.nodes[nid]["token"] = tok
    G.nodes[nid]["aid_set"] = token_occurrences.get(tok, set())

# co-occurrence edges
print("Ð¡Ñ‚Ñ€Ð¾Ð¸Ð¼ co-occurrence Ñ€Ñ‘Ð±Ñ€Ð°...")
for _, row in df.iterrows():
    toks = tokenize(row.get("value", ""))
    node_ids = [token_index[t] for t in toks]
    for i in range(len(node_ids)):
        for j in range(i+1, min(i+1+TOKEN_WINDOW, len(node_ids))):
            u, v = node_ids[i], node_ids[j]
            if u == v: continue
            G.add_edge(u, v, weight=G[u][v]["weight"]+1 if G.has_edge(u, v) else 1.0)

print(f"Ð£Ð·Ð»Ð¾Ð²: {G.number_of_nodes()} | Ð Ñ‘Ð±ÐµÑ€: {G.number_of_edges()}")

print("ÐšÐ¾Ð´Ð¸Ñ€ÑƒÐµÐ¼ Ñ‚Ð¾ÐºÐµÐ½Ñ‹ Ñ‡ÐµÑ€ÐµÐ· SentenceTransformer...")
sbert = SentenceTransformer("all-MiniLM-L6-v2")
token_embeddings = sbert.encode(node_to_token, show_progress_bar=True, normalize_embeddings=True)

# Ð´Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ similarity-edges Ð¼ÐµÐ¶Ð´Ñƒ Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ°Ð¼Ð¸ Ð¸ Ñ‚Ð¾ÐºÐµÐ½Ð°Ð¼Ð¸
question_tokens = list({t for q in dfq["user_qu"].astype(str) for t in tokenize(q)})
q_token_ids = [token_index[t] for t in question_tokens if t in token_index]

if q_token_ids:
    sim = cosine_similarity(token_embeddings[q_token_ids], token_embeddings)
    for i_q, qnid in enumerate(q_token_ids):
        for j_node, s in enumerate(sim[i_q]):
            if j_node != qnid and s >= EDGE_SIM_THRESHOLD:
                G.add_edge(qnid, j_node, weight=G[qnid][j_node]["weight"]+s if G.has_edge(qnid, j_node) else s)

print(f"ÐŸÐ¾ÑÐ»Ðµ Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ñ: {G.number_of_edges()} Ñ€Ñ‘Ð±ÐµÑ€")

print("Ð’Ñ‹Ñ‡Ð¸ÑÐ»ÑÐµÐ¼ Ð¼Ð°Ñ‚Ñ€Ð¸Ñ†Ñƒ Ñ€Ð°ÑÑÑ‚Ð¾ÑÐ½Ð¸Ð¹...")
n = len(G)
dist_mat = np.full((n, n), DIST_MAX, dtype=np.int64)
for i in range(n): dist_mat[i, i] = 0
lengths = dict(nx.all_pairs_shortest_path_length(G, cutoff=DIST_MAX))
for i, d in lengths.items():
    for j, l in d.items():
        dist_mat[i, j] = min(l, DIST_MAX)

# --- Graphormer ÑÐ»Ð¾Ð¸ ---
import math

class SimpleGraphormerLayer(nn.Module):
    def __init__(self, d_model, n_heads=4, max_dist=10):
        super().__init__()
        self.n_heads, self.d_head = n_heads, d_model // n_heads
        self.Wq, self.Wk, self.Wv = nn.Linear(d_model, d_model), nn.Linear(d_model, d_model), nn.Linear(d_model, d_model)
        self.out = nn.Linear(d_model, d_model)
        self.dist_emb = nn.Embedding(max_dist+1, n_heads)
        self.norm1, self.norm2 = nn.LayerNorm(d_model), nn.LayerNorm(d_model)
        self.ff = nn.Sequential(nn.Linear(d_model, 4*d_model), nn.ReLU(), nn.Linear(4*d_model, d_model))

    def forward(self, x, dist):
        N = x.size(0)
        Q, K, V = self.Wq(x), self.Wk(x), self.Wv(x)
        Q, K, V = Q.view(N, self.n_heads, -1), K.view(N, self.n_heads, -1), V.view(N, self.n_heads, -1)
        scores = torch.einsum("ihd,jhd->hij", Q, K)/math.sqrt(self.d_head)
        bias = self.dist_emb(torch.clamp(dist, 0, self.dist_emb.num_embeddings-1)).permute(2,0,1)
        att = F.softmax(scores+bias, dim=-1)
        out = torch.einsum("hij,jhd->ihd", att, V).reshape(N, -1)
        x = self.norm1(x + self.out(out))
        return self.norm2(x + self.ff(x))

class SimpleGraphormer(nn.Module):
    def __init__(self, in_dim, d_model=GRAPHORMER_D_MODEL, n_layers=2):
        super().__init__()
        self.proj = nn.Linear(in_dim, d_model)
        self.layers = nn.ModuleList([SimpleGraphormerLayer(d_model) for _ in range(n_layers)])
    def forward(self, x_np, dist_np):
        x = torch.tensor(x_np, dtype=torch.float32)
        dist = torch.tensor(dist_np, dtype=torch.long)
        x = self.proj(x)
        for layer in self.layers: x = layer(x, dist)
        return x.detach().numpy()

model = SimpleGraphormer(token_embeddings.shape[1])
node_repr = model(token_embeddings, dist_mat)

# mean-pool Ð¿Ð¾ aid
aid_list = list(aid_to_token_ids)
article_reprs = np.zeros((len(aid_list), node_repr.shape[1]))
for i, aid in enumerate(aid_list):
    ids = aid_to_token_ids[aid]
    article_reprs[i] = node_repr[ids].mean(axis=0) if ids else np.zeros(node_repr.shape[1])
aid_to_repr = {aid_list[i]: article_reprs[i] for i in range(len(aid_list))}

print("Ð˜Ñ‰ÐµÐ¼ Ð¾Ñ‚Ð²ÐµÑ‚Ñ‹...")
predicted = []
for _, row in tqdm(dfq.iterrows(), total=len(dfq)):
    q_toks = tokenize(str(row["user_qu"]))
    ids = [token_index[t] for t in q_toks if t in token_index]
    q_vec = node_repr[ids].mean(axis=0) if ids else np.zeros(node_repr.shape[1])
    sims = cosine_similarity(q_vec.reshape(1, -1), article_reprs)[0]
    predicted.append(aid_list[int(np.argmax(sims))])

dfq["predicted_aid"] = predicted
if "aid" in dfq:
    acc = (dfq["predicted_aid"].astype(str) == dfq["aid"].astype(str)).mean()
    print(f"âœ… Accuracy: {acc:.3f}")
dfq.to_csv("dfq_with_predictions.tsv", sep="\t", index=False)

from pyvis.network import Network
from IPython.display import IFrame

print("Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ð¼ ÑƒÐ¿Ñ€Ð¾Ñ‰Ñ‘Ð½Ð½Ñ‹Ð¹ HTML-Ð³Ñ€Ð°Ñ„ (Ñ‚Ð¾Ð»ÑŒÐºÐ¾ 20 ÑƒÐ·Ð»Ð¾Ð² Ð¸ Ñ€Ñ‘Ð±Ñ€Ð° Ð¼ÐµÐ¶Ð´Ñƒ Ð½Ð¸Ð¼Ð¸)...")

# ÑÐ¾Ð·Ð´Ð°Ñ‘Ð¼ Ð¿Ð¾Ð´Ð³Ñ€Ð°Ñ„ Ñ 20 ÑƒÐ·Ð»Ð°Ð¼Ð¸
limited_nodes = list(G.nodes())[:20]
G_sub = G.subgraph(limited_nodes)

# ÑÐ¾Ð·Ð´Ð°Ñ‘Ð¼ Ð²Ð¸Ð·ÑƒÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸ÑŽ
net = Network(height="750px", width="100%", bgcolor="#ffffff", font_color="black",
              notebook=True, cdn_resources="in_line")

# Ð´Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ ÑƒÐ·Ð»Ñ‹
for i, tok in enumerate(G_sub.nodes()):
    net.add_node(int(i), label=str(tok))

# Ð´Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ñ€Ñ‘Ð±Ñ€Ð°
for u, v in G_sub.edges():
    net.add_edge(int(limited_nodes.index(u)), int(limited_nodes.index(v)))

# Ð²Ñ‹Ð²Ð¾Ð´ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ¸
print(f"Ð“Ñ€Ð°Ñ„ ÑÐ¾Ð´ÐµÑ€Ð¶Ð¸Ñ‚ {G_sub.number_of_nodes()} ÑƒÐ·Ð»Ð¾Ð² Ð¸ {G_sub.number_of_edges()} Ñ€Ñ‘Ð±ÐµÑ€.")

# ÑÐ¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð² HTML
html_path = "word_graph_simple.html"
net.write_html(html_path, notebook=False)
print(f"âœ… Ð£Ð¿Ñ€Ð¾Ñ‰Ñ‘Ð½Ð½Ñ‹Ð¹ Ð³Ñ€Ð°Ñ„ ÑÐ¾Ñ…Ñ€Ð°Ð½Ñ‘Ð½: {html_path}")

print(f"Ð“Ñ€Ð°Ñ„ ÑÐ¾Ð´ÐµÑ€Ð¶Ð¸Ñ‚ {G.number_of_nodes()} ÑƒÐ·Ð»Ð¾Ð² Ð¸ {G.number_of_edges()} Ñ€Ñ‘Ð±ÐµÑ€.")

# =====================================
# ðŸ“Š Ð¤Ð¸Ð½Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð²Ñ‹Ð²Ð¾Ð´ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð² Ð¿Ð¾Ð¸ÑÐºÐ° + Top-1 Ð¸ Top-3 Accuracy
# =====================================
from sklearn.metrics.pairwise import cosine_similarity
import textwrap

print("\n" + "="*80)
print("ðŸ“˜ Ð Ð•Ð—Ð£Ð›Ð¬Ð¢ÐÐ¢Ð« ÐŸÐžÐ˜Ð¡ÐšÐ ÐžÐ¢Ð’Ð•Ð¢ÐžÐ’ (Ñ Ð¾Ñ†ÐµÐ½ÐºÐ°Ð¼Ð¸ Ð¸ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ°Ð¼Ð¸)")
print("="*80 + "\n")

TOP_N = 3
SNIPPET_LEN = 100

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼, Ñ‡Ñ‚Ð¾ embeddings ÐµÑÑ‚ÑŒ
if 'article_reprs' not in locals():
    article_reprs = enriched_articles
if 'query_reprs' not in locals():
    query_reprs = enriched_questions

top1_correct = 0
top3_correct = 0
total = len(dfq)

for idx, row in enumerate(dfq.itertuples(), 1):
    q = getattr(row, "user_qu", "")
    true_aid = str(getattr(row, "aid", "")) if "aid" in dfq.columns else None

    # Ð’ÐµÐºÑ‚Ð¾Ñ€ Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ° Ð¸ ÐºÐ¾ÑÐ¸Ð½ÑƒÑÐ½Ð°Ñ ÑÑ…Ð¾Ð¶ÐµÑÑ‚ÑŒ ÑÐ¾ ÑÑ‚Ð°Ñ‚ÑŒÑÐ¼Ð¸
    q_vec = query_reprs[idx-1]
    sims = cosine_similarity(q_vec.reshape(1, -1), article_reprs)[0]
    top_idx = np.argsort(sims)[-TOP_N:][::-1]

    print(f"ðŸ”¹ Ð’Ð¾Ð¿Ñ€Ð¾Ñ: {q}")
    print(f"{'-'*90}")

    # Ð’Ñ‹Ð²Ð¾Ð´Ð¸Ð¼ top-3 ÑÑ‚Ð°Ñ‚ÑŒÐ¸ Ñ Ð¾Ñ†ÐµÐ½ÐºÐ°Ð¼Ð¸
    for rank, i in enumerate(top_idx, 1):
        aid = str(aid_list[i])
        score = sims[i]
        snippet = df[df["aid"] == aid]["value"].astype(str).iloc[0][:SNIPPET_LEN]
        print(f"#{rank}: AID={aid} | SCORE={score:.3f} | {snippet}...")

    # ÐžÐ¶Ð¸Ð´Ð°ÐµÐ¼Ð°Ñ ÑÑ‚Ð°Ñ‚ÑŒÑ
    if true_aid and true_aid in df["aid"].astype(str).values:
        true_text = df[df["aid"] == true_aid]["value"].astype(str).iloc[0][:SNIPPET_LEN]
        print(f"\nâœ… ÐžÐ¶Ð¸Ð´Ð°ÐµÐ¼Ð°Ñ ÑÑ‚Ð°Ñ‚ÑŒÑ: AID={true_aid} | {true_text}...")
    else:
        print("\nâš ï¸ ÐžÐ¶Ð¸Ð´Ð°ÐµÐ¼Ð°Ñ ÑÑ‚Ð°Ñ‚ÑŒÑ Ð¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ð²ÑƒÐµÑ‚ Ð² df")

    # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ñ‚Ð¾Ñ‡Ð½Ð¾ÑÑ‚ÑŒ
    predicted_top = [str(aid_list[i]) for i in top_idx]
    if len(predicted_top) > 0 and predicted_top[0] == true_aid:
        top1_correct += 1
    if true_aid in predicted_top:
        top3_correct += 1

    print("\n" + "-"*90 + "\n")

# ÐœÐµÑ‚Ñ€Ð¸ÐºÐ¸
acc1 = top1_correct / total if total > 0 else 0
acc3 = top3_correct / total if total > 0 else 0
print(f"ðŸŽ¯ Top-1 Accuracy: {acc1:.3f} ({top1_correct}/{total})")
print(f"ðŸŽ¯ Top-3 Accuracy: {acc3:.3f} ({top3_correct}/{total})")

